# Predictive maintenance for reefer containers


## Getting Started

The model uses the generated data from three scenarios: 1) when the container's door is open for a longer time - this gives a false positive that maintainence is required 2) when sensors are malfunctioning, it records arbitrary readings, 3) when the readings are normal. We have currently trained our model on 3000 datapoints from the three scenarios above. 

### Code execution

The simulator continuously generates container metrics, publishes them to Kafka `containers` topics and run the predictMaintainence.ipynb to predict if maintainence is sought at this point in time. 

## Dataset

We do not have datset for reefer containers now. We have manually created the training and testing data. The container metrics generated by the simulator is also manually created. The dataset has the following format.=========================Timestamp, ID, Temperature(celsius), Target_Temperature(celsius), Amp, CumulativePowerConsumption, ContentType, Humidity, CO2, Door_Open,Maintainence_Required, Defrost_Cycle=====================

## Model description

We are using Machine Learning supervised learning here. There are two types of supervised learning - 1) Classification: Predict a categorical response, 2) Regression: Predict a continuous response

### Linear regression

Pros: 1) Fast 2) No tuning required 3) Highly interpretable 4) Well-understood

Cons: 1) Unlikely to produce the best predictive accuracy 2) Presumes a linear relationship between the features and response 3) If the relationship is highly non-linear as with many scenarios, linear relationship will not effectively model the relationship and its prediction would not be accurate

### Naive Bayes classification

Naive Bayes is a probabilistic classifier inspired by the Bayes theorem under a simple assumption which is the attributes are conditionally independent.

The classification is conducted by deriving the maximum posterior which is the maximal P(Ci|X) with the above assumption applying to Bayes theorem. This assumption greatly reduces the computational cost by only counting the class distribution. Even though the assumption is not valid in most cases since the attributes are dependent, surprisingly Naive Bayes has able to perform impressively.

Naive Bayes is a very simple algorithm to implement and good results have obtained in most cases. It can be easily scalable to larger datasets since it takes linear time, rather than by expensive iterative approximation as used for many other types of classifiers.

Naive Bayes can suffer from a problem called the zero probability problem. When the conditional probability is zero for a particular attribute, it fails to give a valid prediction. This needs to be fixed explicitly using a Laplacian estimator.



## Model evaluation

We are using Root Mean Squared Error (RMSE) for evaluating the model performance.

Root Mean Squared Error (RMSE) is the square root of the mean of the squared errors.

Classification does better here as the scenarion is more of a classification problem.

## References

http://www.sfb637.uni-bremen.de/pubdb/repository/SFB637-B6-10-011-IC.pdf
https://www.omega.com/technical-learning/temperature-monitoring-during-transportation.html
https://cargostore.com/blog/what-are-reefer-containers/
https://towardsdatascience.com/machine-learning-classifiers-a5cc4e1b0623

