{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Container management microservice Abstract This project is part of the container shipment implementation solution, and address the Reefer container management microservice implmentaiton. You can read more about the end to end solution in this chapter. TL;TR The goal of this Container management service is to support the reefer containers inventory management and to process all the events related to the container entity. We are proposing three different implementations: Springboot and spring kafka template and spring postgreSQL. See this note. Python with Flask and Confluent Kafka API for Python. See this description. - NOT DONE YET Microprofile 2.2 using Kafka Streams. See this description. - NOT DONE YET We are demonstrating in this project how to transform an event storming analysis to an event-driven microservice implementation and how to address 'reversibility' between the different platform. The service is packaged via dockerfile, and helm release is defined to deploy to kubernetes. Analysis We have a dedicated article on how to transform event storming analysis to microservice, here . Component view As the service needs to offer some basic APIs while consuming and producing events, the code has at least three main components: a kafka consumer, a kafka producer, and a HTTP server exposing the REST APIs. The following diagram illustrates the components involved in this container manager microservice: A first component is responsible to define and expose APIs via RESTful protocol. It uses the services to delegate business logic implementation. The service component(s) addresses the business logic implementation and references data access object, and event producer. The event handler is a kafka consumer which runs continuously to get container events from the container topic. It invokes the service component. The event producer, produces events of interest to the business function The DAO implement the persistence for the data received as part of the event payload, or API. Container inventory We are providing a tool to publish container created events to the Kafka containers topic. The python code is in the root project refarch-kc under the itg-tests/ContainersPython folder . The addContainer.sh uses our Python docker image to create a container and send it to kafka. When Kafka runs on IBM Cloud use: ./addContainer.sh IBMCLOUD If you want to run using a kafka running on your computer with docker-compose: ./addContainer.sh LOCAL ./addContainer.sh MINIKUBE or with minikube: And for ICP ./addContainer.sh ICP The trace from the python execution looks like: Create container {'containerID': 'itg-C02', 'timestamp': 1559868907, 'type': 'ContainerAdded', 'payload': {'containerID': 'itg-C02', 'type': 'Reefer', 'status': 'Empty', 'latitude': 37.8, 'longitude': -122.25, 'capacity': 110, 'brand': 'itg-brand'}} Message delivered to containers [0] While on the running Java microservice, you will could see the service consumed the message: INFO 47 --- [ingConsumer-C-1] c.l.k.c.kafka.ContainerConsumer : Received container event: {\"containerID\": \"itg-C02\", \"timestamp\": 1559868907, \"type\": \"ContainerAdded\", \"payload\": {\"containerID\": \"itg-C02\", \"type\": \"Reefer\", \"status\": \"Empty\", \"latitude\": 37.8, \"longitude\": -122.25, \"capacity\": 110, \"brand\": \"itg-brand\"}} It is also possible to start the python environment with Docker, and then inside the container bash session use python as you will do in your own computer. source ../../scripts/setenv.sh docker run -e KAFKA_BROKERS = $KAFKA_BROKERS -v $( pwd ) :/home --network = docker_default -ti ibmcase/python bash root@2f049cb7b4f2:/ cd home root@2f049cb7b4f2:/ python ProduceContainerCreatedEvent.py Assign container to order The implementation will search the list of containers closed to the source location. We simplify the implementation by assuming mapping container (longitude, latitude) position to be in an area close to the harbor in the same region as the pickup location. We do not manage the time when the container will be there. We assume containers are at the location at the time of the order is processed, is close enought to the time of the pickup. In a future iteration, we may fine tune that if we can make it simple. The output of this assignment processing is an event to the orders topic. See implementation details in this note . Compendium Postgresql tutorial psql commands Spring boot kafka documentation","title":"Introduction"},{"location":"#container-management-microservice","text":"Abstract This project is part of the container shipment implementation solution, and address the Reefer container management microservice implmentaiton. You can read more about the end to end solution in this chapter.","title":"Container management microservice"},{"location":"#tltr","text":"The goal of this Container management service is to support the reefer containers inventory management and to process all the events related to the container entity. We are proposing three different implementations: Springboot and spring kafka template and spring postgreSQL. See this note. Python with Flask and Confluent Kafka API for Python. See this description. - NOT DONE YET Microprofile 2.2 using Kafka Streams. See this description. - NOT DONE YET We are demonstrating in this project how to transform an event storming analysis to an event-driven microservice implementation and how to address 'reversibility' between the different platform. The service is packaged via dockerfile, and helm release is defined to deploy to kubernetes.","title":"TL;TR"},{"location":"#analysis","text":"We have a dedicated article on how to transform event storming analysis to microservice, here .","title":"Analysis"},{"location":"#component-view","text":"As the service needs to offer some basic APIs while consuming and producing events, the code has at least three main components: a kafka consumer, a kafka producer, and a HTTP server exposing the REST APIs. The following diagram illustrates the components involved in this container manager microservice: A first component is responsible to define and expose APIs via RESTful protocol. It uses the services to delegate business logic implementation. The service component(s) addresses the business logic implementation and references data access object, and event producer. The event handler is a kafka consumer which runs continuously to get container events from the container topic. It invokes the service component. The event producer, produces events of interest to the business function The DAO implement the persistence for the data received as part of the event payload, or API.","title":"Component view"},{"location":"#container-inventory","text":"We are providing a tool to publish container created events to the Kafka containers topic. The python code is in the root project refarch-kc under the itg-tests/ContainersPython folder . The addContainer.sh uses our Python docker image to create a container and send it to kafka. When Kafka runs on IBM Cloud use: ./addContainer.sh IBMCLOUD If you want to run using a kafka running on your computer with docker-compose: ./addContainer.sh LOCAL ./addContainer.sh MINIKUBE or with minikube: And for ICP ./addContainer.sh ICP The trace from the python execution looks like: Create container {'containerID': 'itg-C02', 'timestamp': 1559868907, 'type': 'ContainerAdded', 'payload': {'containerID': 'itg-C02', 'type': 'Reefer', 'status': 'Empty', 'latitude': 37.8, 'longitude': -122.25, 'capacity': 110, 'brand': 'itg-brand'}} Message delivered to containers [0] While on the running Java microservice, you will could see the service consumed the message: INFO 47 --- [ingConsumer-C-1] c.l.k.c.kafka.ContainerConsumer : Received container event: {\"containerID\": \"itg-C02\", \"timestamp\": 1559868907, \"type\": \"ContainerAdded\", \"payload\": {\"containerID\": \"itg-C02\", \"type\": \"Reefer\", \"status\": \"Empty\", \"latitude\": 37.8, \"longitude\": -122.25, \"capacity\": 110, \"brand\": \"itg-brand\"}} It is also possible to start the python environment with Docker, and then inside the container bash session use python as you will do in your own computer. source ../../scripts/setenv.sh docker run -e KAFKA_BROKERS = $KAFKA_BROKERS -v $( pwd ) :/home --network = docker_default -ti ibmcase/python bash root@2f049cb7b4f2:/ cd home root@2f049cb7b4f2:/ python ProduceContainerCreatedEvent.py","title":"Container inventory"},{"location":"#assign-container-to-order","text":"The implementation will search the list of containers closed to the source location. We simplify the implementation by assuming mapping container (longitude, latitude) position to be in an area close to the harbor in the same region as the pickup location. We do not manage the time when the container will be there. We assume containers are at the location at the time of the order is processed, is close enought to the time of the pickup. In a future iteration, we may fine tune that if we can make it simple. The output of this assignment processing is an event to the orders topic. See implementation details in this note .","title":"Assign container to order"},{"location":"#compendium","text":"Postgresql tutorial psql commands Spring boot kafka documentation","title":"Compendium"},{"location":"ES2DDD2MS/","text":"From Event Storming to DDD to Microservice Analysis Combining a domain-driven design and event storming we identified the following events: ContainerAddedToInventory, ContainerRemovedFromInventory ContainerAtLocation ContainerOnMaintenance, ContainerOffMaintenance, ContainerAssignedToOrder, ContainerReleasedFromOrder ContainerGoodLoaded, ContainerGoodUnLoaded ContainerOnShip, ContainerOffShip ContainerOnTruck, ContainerOffTruck Which leads to the following commands, actors and events: Add Reefer container to inventory Assign container to order Release container from order Remove Reefer container to inventory Set container in maintenance Set container off maintenance The main entity is the Container, with a unique ID, the containerID. We assume this ID is created by another application as a physical representation of this identifier need to physically exists on the container itself. The bounded context for the application is the container shipment context. With sub decomposition including the container and the order contexts: A detail view of container bounded context may look like in the following figure: This is a first version of this bounded context, and it will evolve over time. We map this bounded context to the java package ibm.labs.kc.containermgr . The source tree would be further divided according to architectural responsibilities. Commands are map to exposed APIs. The features to support in each possible implementation are: A REST API end point calleable from other services to get inventory content, a container by identifier, and to create a new container. A Kafka consumer to get orderCreated event published in the orders topic: the code will look at the pickup location and search in the container inventory the containers close to this location. A kafka consumer to get any container events from the containers topic.","title":"Analysis and design considerations"},{"location":"ES2DDD2MS/#from-event-storming-to-ddd-to-microservice","text":"","title":"From Event Storming to DDD to Microservice"},{"location":"ES2DDD2MS/#analysis","text":"Combining a domain-driven design and event storming we identified the following events: ContainerAddedToInventory, ContainerRemovedFromInventory ContainerAtLocation ContainerOnMaintenance, ContainerOffMaintenance, ContainerAssignedToOrder, ContainerReleasedFromOrder ContainerGoodLoaded, ContainerGoodUnLoaded ContainerOnShip, ContainerOffShip ContainerOnTruck, ContainerOffTruck Which leads to the following commands, actors and events: Add Reefer container to inventory Assign container to order Release container from order Remove Reefer container to inventory Set container in maintenance Set container off maintenance The main entity is the Container, with a unique ID, the containerID. We assume this ID is created by another application as a physical representation of this identifier need to physically exists on the container itself. The bounded context for the application is the container shipment context. With sub decomposition including the container and the order contexts: A detail view of container bounded context may look like in the following figure: This is a first version of this bounded context, and it will evolve over time. We map this bounded context to the java package ibm.labs.kc.containermgr . The source tree would be further divided according to architectural responsibilities. Commands are map to exposed APIs. The features to support in each possible implementation are: A REST API end point calleable from other services to get inventory content, a container by identifier, and to create a new container. A Kafka consumer to get orderCreated event published in the orders topic: the code will look at the pickup location and search in the container inventory the containers close to this location. A kafka consumer to get any container events from the containers topic.","title":"Analysis"},{"location":"cicd/","text":"Continuous delivery For the SpringContainerMS service, we are using one of the existing toolchains of IBM Cloud. Creating a Toolchain To create one toolchain, go to search bar, type toolchains and it will take you to the screen below: Choose Develop a Kubernetes app with Helm template and it will take you to the creation page. Enter the Toolchain Name. Choose a Region. Select a Resource group. Scroll down and you will see some more details. Fill them out as well. Mention the server. You can clone or use an existing repository. Provide the repo URL. Give the repo a name. Now go to the Delivery Pipeline Section. Provide an app name. Create IBM Cloud API key. Choose a container registry region and namespace. Select a cluster region, resource group, cluster name and cluster namespace. Now go ahead and click create . Tool Integration You can add additional tools to your pipeline as follows. For this application, we are adding the github integration. ! [ Tool Integration ](. / images / Add_tool_integration . png ) And then add the details of your repository. ![Tool integartion details](./images/integration_configuration.png) Based on your requirements you can also add other tools. Since we don't need Git provided by the default Toolchain, we deleted it. Toolchain Once configured, our toolchain looks like below. ![Toolchain](./images/containerms_toolchain.png) Clicking on Issues takes you to the Github Issues page. Clicking on Github takes you to the Github repository. Delivery Pipeline is the most important piece where we defined our Build, Validate and Prod stages as follows. If you click on the settings of each stage, you will have an option called Configure Stage . Using this, you can modify the input, jobs and environment variables based on your requirements. Also, in the validate phase, vulnerability advisor is one of the security features provided by IBM Cloud. It scans the security status of the docker images that are pushed to the registry namespace. When the image is added to the namespace, it will be scanned automatically by the vulnerability advisor to detect the security issues and potential vulnerabilities. It will generate a report suggesting the fixes. You can also add exemptions based on your requirements. Fixing the issues that are reported can help you to secure your IBM Cloud infrastructure. Note One more thing to know is whenever there is any change committed to your github repo, the toolchain will trigger the delivery pipeline automatically and the app with the latest changes will be deployed to your environment.","title":"Continuous delivery"},{"location":"cicd/#continuous-delivery","text":"For the SpringContainerMS service, we are using one of the existing toolchains of IBM Cloud.","title":"Continuous delivery"},{"location":"cicd/#creating-a-toolchain","text":"To create one toolchain, go to search bar, type toolchains and it will take you to the screen below: Choose Develop a Kubernetes app with Helm template and it will take you to the creation page. Enter the Toolchain Name. Choose a Region. Select a Resource group. Scroll down and you will see some more details. Fill them out as well. Mention the server. You can clone or use an existing repository. Provide the repo URL. Give the repo a name. Now go to the Delivery Pipeline Section. Provide an app name. Create IBM Cloud API key. Choose a container registry region and namespace. Select a cluster region, resource group, cluster name and cluster namespace. Now go ahead and click create .","title":"Creating a Toolchain"},{"location":"cicd/#tool-integration","text":"You can add additional tools to your pipeline as follows. For this application, we are adding the github integration. ! [ Tool Integration ](. / images / Add_tool_integration . png ) And then add the details of your repository. ![Tool integartion details](./images/integration_configuration.png) Based on your requirements you can also add other tools. Since we don't need Git provided by the default Toolchain, we deleted it.","title":"Tool Integration"},{"location":"cicd/#toolchain","text":"Once configured, our toolchain looks like below. ![Toolchain](./images/containerms_toolchain.png) Clicking on Issues takes you to the Github Issues page. Clicking on Github takes you to the Github repository. Delivery Pipeline is the most important piece where we defined our Build, Validate and Prod stages as follows. If you click on the settings of each stage, you will have an option called Configure Stage . Using this, you can modify the input, jobs and environment variables based on your requirements. Also, in the validate phase, vulnerability advisor is one of the security features provided by IBM Cloud. It scans the security status of the docker images that are pushed to the registry namespace. When the image is added to the namespace, it will be scanned automatically by the vulnerability advisor to detect the security issues and potential vulnerabilities. It will generate a report suggesting the fixes. You can also add exemptions based on your requirements. Fixing the issues that are reported can help you to secure your IBM Cloud infrastructure. Note One more thing to know is whenever there is any change committed to your github repo, the toolchain will trigger the delivery pipeline automatically and the app with the latest changes will be deployed to your environment.","title":"Toolchain"},{"location":"deployment/","text":"Environment pre-requisites Deployment prerequisites Regardless of specific deployment targets (OCP, IKS, k8s), the following prerequisite Kubernetes artifacts need to be created to support the deployments of application components. These artifacts need to be created once per unique deployment of the entire application and can be shared between application components in the same overall application deployment. Create kafka-brokers ConfigMap Command: kubectl create configmap kafka-brokers --from-literal=brokers='<replace with comma-separated list of brokers>' -n <namespace> Example: kubectl create configmap kafka-brokers --from-literal=brokers='broker-3-j7fxtxtp5fs84205.kafka.svc01.us-south.eventstreams.cloud.ibm.com:9093,broker-2-j7fxtxtp5fs84205.kafka.svc01.us-south.eventstreams.cloud.ibm.com:9093,broker-1-j7fxtxtp5fs84205.kafka.svc01.us-south.eventstreams.cloud.ibm.com:9093,broker-5-j7fxtxtp5fs84205.kafka.svc01.us-south.eventstreams.cloud.ibm.com:9093,broker-0-j7fxtxtp5fs84205.kafka.svc01.us-south.eventstreams.cloud.ibm.com:9093,broker-4-j7fxtxtp5fs84205.kafka.svc01.us-south.eventstreams.cloud.ibm.com:9093' -n eda-refarch Create optional eventstreams-apikey Secret, if you are using Event Streams as your Kafka broker provider Command: kubectl create secret generic eventstreams-apikey --from-literal=binding='<replace with api key>' -n <namespace> Example: kubectl create secret generic eventstreams-apikey --from-literal=binding='z...12345...notanactualkey...67890...a' -n eda-refarch If you are using Event Streams as your Kafka broker provider and it is deployed via the IBM Cloud Pak for Integration (ICP4I), you will need to create an additional Secret to store the generated Certificates & Truststores. From the \"Connect to this cluster\" tab on the landing page of your Event Streams installation, download both the Java truststore and the PEM certificate . Create the Java truststore Secret: Command: kubectl create secret generic <secret-name> --from-file=/path/to/downloaded/file.jks Example: kubectl create secret generic es-truststore-jks --from-file=/Users/osowski/Downloads/es-cert.jks Create the PEM certificate Secret: Command: kubectl create secret generic <secret-name> --from-file=/path/to/downloaded/file.pem Example: kubectl create secret generic es-ca-pemfile --from-file=/Users/osowski/Downloads/es-cert.pem Create postgresql-ca-pem Secret Command: kubectl create secret generic postgresql-ca-pem --from-literal=binding='<replace with postgresql ca-pem certificate>' -n <namespace> Example: kubectl create secret generic postgresql-ca-pem --from-literal=binding='-----BEGIN CERTIFICATE-----...MIIEczCCA1ugAw...-----END CERTIFICATE-----' -n eda-refarch Create postgresql-url Secret Command: kubectl create secret generic postgresql-url --from-literal=binding='<replace with postgresql url>' -n <namespace> Example: kubectl create secret generic postgresql-url --from-literal=binding='jdbc:postgresql://bd2...' -n eda-refarch Create postgresql-user Secret Command: kubectl create secret generic postgresql-user --from-literal=binding='<replace with postgresql user>' -n <namespace> Example: kubectl create secret generic postgresql-user --from-literal=binding='postgresqlUser' -n eda-refarch Create postgresql-pwd Secret Command: kubectl create secret generic postgresql-pwd --from-literal=binding='<replace with postgresql password>' -n <namespace> Example: kubectl create secret generic postgresql-pwd --from-literal=binding='postgresqlPassword' -n eda-refarch Local deployment Prepare minikube or docker edge / kubernetes The solution can run on minikube, and once you have a minikube or docker community edition with kubernetes enabled, you can follow the instructions from this note . Or use docker compose See how to use docker-compose to run backend service and the solution in this article. Public cloud deployment Prepare an IBM Kubernetes service cluster See this note on how to deploy a IKS cluster . Define a docker image private registry using this note. Define a namespace to deploy your solution. We used greencompute as namespace but can be anything. kubectl create namespace greencompute Prepare Event Stream service on IBM Cloud See this note to deploy IBM kafka based product as a service on IBM Cloud . Get API Key and brokers URL. Then defines secrets for the api key as described in this note. Add the private token to access the docker image registry, see details in this note. Prepare Postgresql service on IBM Cloud Follow product documentation. Then add secrets for postgresql by following the procedure described in this note. Private Cloud For IBM Cloud Private deployments go to this article. Deploy to OpenShift Container Platform (OCP) Deploy to OCP 3.11 Cross-component deployment prerequisites: (needs to be done once per unique deployment of the entire application) 1. If desired, create a non-default Service Account for usage of deploying and running the K Container reference implementation. This will become more important in future iterations, so it's best to start small: - Command: oc create serviceaccount -n <target-namespace> kcontainer-runtime - Example: oc create serviceaccount -n eda-refarch kcontainer-runtime 2. The target Service Account needs to be allowed to run containers as anyuid for the time being: - Command: oc adm policy add-scc-to-user anyuid -z <service-account-name> -n <target-namespace> - Example: oc adm policy add-scc-to-user anyuid -z kcontainer-runtime -n eda-refarch - NOTE: This requires cluster-admin level privileges. Perform the following for the SpringContainerMS microservice: 1. Build and push the Docker image by one of the two options below: - Create a Jenkins project, pointing to the remote GitHub repository for the voyages-ms microservice, and manually creating the necessary parameters. Refer to the individual microservice's Jenkinsfile.NoKubernetesPlugin for appropriate parameter values. - Manually build the Docker image and push it to a registry that is accessible from your cluster (Docker Hub, IBM Cloud Container Registry, manually deployed Quay instance): - docker build -t <private-registry>/<image-namespace>/kc-spring-container-ms:latest SpringContainerMS/ - docker login <private-registry> - docker push <private-registry>/<image-namespace>/kc-spring-container-ms:latest 3. Generate application YAMLs via helm template : - Parameters: - --set image.repository=<private-registry>/<image-namespace>/<image-repository> - --set image.pullSecret=<private-registry-pullsecret> (optional or set to blank) - --set kafka.brokersConfigMap=<kafka brokers ConfigMap name> - --set eventstreams.enabled=(true/false) ( true when connecting to Event Streams of any kind, false when connecting to Kafka directly) - --set eventstreams.apikeyConfigMap=<kafka api key Secret name> - --set eventstreams.truststoreRequired=(true/false) ( true when connecting to Event Streams via ICP4I) - --set eventstreams.truststoreSecret=<eventstreams jks file secret name> (only used when connecting to Event Streams via ICP4I) - --set eventstreams.truststorePassword=<eventstreams jks password> (only used when connecting to Event Streams via ICP4I) - --set postgresql.capemRequired=(true/false) ( true when connecting to Postgresql Services requiring SSL and CA PEM-secured communication) - --set postgresql.capemSecret=<postgresql CA pem certificate Secret name> - --set postgresql.urlSecret=<postgresql url Secret name> - --set postgresql.userSecret=<postgresql user Secret name> - --set postgresql.passwordSecret=<postgresql password Secret name> - --set serviceAccountName=<service-account-name> - --namespace <target-namespace> - --output-dir <local-template-directory> - Example using Event Streams via ICP4I: helm template --set image.repository = rhos-quay.internal-network.local/browncompute/kc-spring-container-ms --set image.pullSecret = --set kafka.brokersConfigMap = es-kafka-brokers --set eventstreams.enabled = true --set eventstreams.apikeyConfigMap = es-eventstreams-apikey --set eventstreams.truststoreRequired = true --set eventstreams.truststoreSecret = es-truststore-jks --set eventstreams.truststorePassword = password --set postgresql.capemRequired = true --set postgresql.capemSecret = postgresql-ca-pem --set postgresql.urlSecret = postgresql-url --set postgresql.userSecret = postgresql-user --set postgresql.passwordSecret = postgresql-pwd --set serviceAccountName = kcontainer-runtime --output-dir templates --namespace eda-refarch chart/springcontainerms - Example using Event Streams hosted on IBM Cloud: helm template --set image.repository = rhos-quay.internal-network.local/browncompute/kc-spring-container-ms --set image.pullSecret = --set kafka.brokersConfigMap = es-kafka-brokers --set eventstreams.enabled = true --set eventstreams.apikeyConfigMap = es-eventstreams-apikey --set postgresql.capemRequired = true --set postgresql.capemSecret = postgresql-ca-pem --set postgresql.urlSecret = postgresql-url --set postgresql.userSecret = postgresql-user --set postgresql.passwordSecret = postgresql-pwd --set serviceAccountName = kcontainer-runtime --output-dir templates --namespace eda-refarch chart/springcontainerms Deploy application using oc apply : oc apply -f templates/springcontainerms/templates","title":"Pre-requisites"},{"location":"deployment/#environment-pre-requisites","text":"","title":"Environment pre-requisites"},{"location":"deployment/#deployment-prerequisites","text":"Regardless of specific deployment targets (OCP, IKS, k8s), the following prerequisite Kubernetes artifacts need to be created to support the deployments of application components. These artifacts need to be created once per unique deployment of the entire application and can be shared between application components in the same overall application deployment. Create kafka-brokers ConfigMap Command: kubectl create configmap kafka-brokers --from-literal=brokers='<replace with comma-separated list of brokers>' -n <namespace> Example: kubectl create configmap kafka-brokers --from-literal=brokers='broker-3-j7fxtxtp5fs84205.kafka.svc01.us-south.eventstreams.cloud.ibm.com:9093,broker-2-j7fxtxtp5fs84205.kafka.svc01.us-south.eventstreams.cloud.ibm.com:9093,broker-1-j7fxtxtp5fs84205.kafka.svc01.us-south.eventstreams.cloud.ibm.com:9093,broker-5-j7fxtxtp5fs84205.kafka.svc01.us-south.eventstreams.cloud.ibm.com:9093,broker-0-j7fxtxtp5fs84205.kafka.svc01.us-south.eventstreams.cloud.ibm.com:9093,broker-4-j7fxtxtp5fs84205.kafka.svc01.us-south.eventstreams.cloud.ibm.com:9093' -n eda-refarch Create optional eventstreams-apikey Secret, if you are using Event Streams as your Kafka broker provider Command: kubectl create secret generic eventstreams-apikey --from-literal=binding='<replace with api key>' -n <namespace> Example: kubectl create secret generic eventstreams-apikey --from-literal=binding='z...12345...notanactualkey...67890...a' -n eda-refarch If you are using Event Streams as your Kafka broker provider and it is deployed via the IBM Cloud Pak for Integration (ICP4I), you will need to create an additional Secret to store the generated Certificates & Truststores. From the \"Connect to this cluster\" tab on the landing page of your Event Streams installation, download both the Java truststore and the PEM certificate . Create the Java truststore Secret: Command: kubectl create secret generic <secret-name> --from-file=/path/to/downloaded/file.jks Example: kubectl create secret generic es-truststore-jks --from-file=/Users/osowski/Downloads/es-cert.jks Create the PEM certificate Secret: Command: kubectl create secret generic <secret-name> --from-file=/path/to/downloaded/file.pem Example: kubectl create secret generic es-ca-pemfile --from-file=/Users/osowski/Downloads/es-cert.pem Create postgresql-ca-pem Secret Command: kubectl create secret generic postgresql-ca-pem --from-literal=binding='<replace with postgresql ca-pem certificate>' -n <namespace> Example: kubectl create secret generic postgresql-ca-pem --from-literal=binding='-----BEGIN CERTIFICATE-----...MIIEczCCA1ugAw...-----END CERTIFICATE-----' -n eda-refarch Create postgresql-url Secret Command: kubectl create secret generic postgresql-url --from-literal=binding='<replace with postgresql url>' -n <namespace> Example: kubectl create secret generic postgresql-url --from-literal=binding='jdbc:postgresql://bd2...' -n eda-refarch Create postgresql-user Secret Command: kubectl create secret generic postgresql-user --from-literal=binding='<replace with postgresql user>' -n <namespace> Example: kubectl create secret generic postgresql-user --from-literal=binding='postgresqlUser' -n eda-refarch Create postgresql-pwd Secret Command: kubectl create secret generic postgresql-pwd --from-literal=binding='<replace with postgresql password>' -n <namespace> Example: kubectl create secret generic postgresql-pwd --from-literal=binding='postgresqlPassword' -n eda-refarch","title":"Deployment prerequisites"},{"location":"deployment/#local-deployment","text":"","title":"Local deployment"},{"location":"deployment/#prepare-minikube-or-docker-edge-kubernetes","text":"The solution can run on minikube, and once you have a minikube or docker community edition with kubernetes enabled, you can follow the instructions from this note .","title":"Prepare minikube or docker edge / kubernetes"},{"location":"deployment/#or-use-docker-compose","text":"See how to use docker-compose to run backend service and the solution in this article.","title":"Or use docker compose"},{"location":"deployment/#public-cloud-deployment","text":"","title":"Public cloud deployment"},{"location":"deployment/#prepare-an-ibm-kubernetes-service-cluster","text":"See this note on how to deploy a IKS cluster . Define a docker image private registry using this note. Define a namespace to deploy your solution. We used greencompute as namespace but can be anything. kubectl create namespace greencompute","title":"Prepare an IBM Kubernetes service cluster"},{"location":"deployment/#prepare-event-stream-service-on-ibm-cloud","text":"See this note to deploy IBM kafka based product as a service on IBM Cloud . Get API Key and brokers URL. Then defines secrets for the api key as described in this note. Add the private token to access the docker image registry, see details in this note.","title":"Prepare Event Stream service on IBM Cloud"},{"location":"deployment/#prepare-postgresql-service-on-ibm-cloud","text":"Follow product documentation. Then add secrets for postgresql by following the procedure described in this note.","title":"Prepare Postgresql service  on IBM Cloud"},{"location":"deployment/#private-cloud","text":"For IBM Cloud Private deployments go to this article.","title":"Private Cloud"},{"location":"deployment/#deploy-to-openshift-container-platform-ocp","text":"","title":"Deploy to OpenShift Container Platform (OCP)"},{"location":"deployment/#deploy-to-ocp-311","text":"Cross-component deployment prerequisites: (needs to be done once per unique deployment of the entire application) 1. If desired, create a non-default Service Account for usage of deploying and running the K Container reference implementation. This will become more important in future iterations, so it's best to start small: - Command: oc create serviceaccount -n <target-namespace> kcontainer-runtime - Example: oc create serviceaccount -n eda-refarch kcontainer-runtime 2. The target Service Account needs to be allowed to run containers as anyuid for the time being: - Command: oc adm policy add-scc-to-user anyuid -z <service-account-name> -n <target-namespace> - Example: oc adm policy add-scc-to-user anyuid -z kcontainer-runtime -n eda-refarch - NOTE: This requires cluster-admin level privileges. Perform the following for the SpringContainerMS microservice: 1. Build and push the Docker image by one of the two options below: - Create a Jenkins project, pointing to the remote GitHub repository for the voyages-ms microservice, and manually creating the necessary parameters. Refer to the individual microservice's Jenkinsfile.NoKubernetesPlugin for appropriate parameter values. - Manually build the Docker image and push it to a registry that is accessible from your cluster (Docker Hub, IBM Cloud Container Registry, manually deployed Quay instance): - docker build -t <private-registry>/<image-namespace>/kc-spring-container-ms:latest SpringContainerMS/ - docker login <private-registry> - docker push <private-registry>/<image-namespace>/kc-spring-container-ms:latest 3. Generate application YAMLs via helm template : - Parameters: - --set image.repository=<private-registry>/<image-namespace>/<image-repository> - --set image.pullSecret=<private-registry-pullsecret> (optional or set to blank) - --set kafka.brokersConfigMap=<kafka brokers ConfigMap name> - --set eventstreams.enabled=(true/false) ( true when connecting to Event Streams of any kind, false when connecting to Kafka directly) - --set eventstreams.apikeyConfigMap=<kafka api key Secret name> - --set eventstreams.truststoreRequired=(true/false) ( true when connecting to Event Streams via ICP4I) - --set eventstreams.truststoreSecret=<eventstreams jks file secret name> (only used when connecting to Event Streams via ICP4I) - --set eventstreams.truststorePassword=<eventstreams jks password> (only used when connecting to Event Streams via ICP4I) - --set postgresql.capemRequired=(true/false) ( true when connecting to Postgresql Services requiring SSL and CA PEM-secured communication) - --set postgresql.capemSecret=<postgresql CA pem certificate Secret name> - --set postgresql.urlSecret=<postgresql url Secret name> - --set postgresql.userSecret=<postgresql user Secret name> - --set postgresql.passwordSecret=<postgresql password Secret name> - --set serviceAccountName=<service-account-name> - --namespace <target-namespace> - --output-dir <local-template-directory> - Example using Event Streams via ICP4I: helm template --set image.repository = rhos-quay.internal-network.local/browncompute/kc-spring-container-ms --set image.pullSecret = --set kafka.brokersConfigMap = es-kafka-brokers --set eventstreams.enabled = true --set eventstreams.apikeyConfigMap = es-eventstreams-apikey --set eventstreams.truststoreRequired = true --set eventstreams.truststoreSecret = es-truststore-jks --set eventstreams.truststorePassword = password --set postgresql.capemRequired = true --set postgresql.capemSecret = postgresql-ca-pem --set postgresql.urlSecret = postgresql-url --set postgresql.userSecret = postgresql-user --set postgresql.passwordSecret = postgresql-pwd --set serviceAccountName = kcontainer-runtime --output-dir templates --namespace eda-refarch chart/springcontainerms - Example using Event Streams hosted on IBM Cloud: helm template --set image.repository = rhos-quay.internal-network.local/browncompute/kc-spring-container-ms --set image.pullSecret = --set kafka.brokersConfigMap = es-kafka-brokers --set eventstreams.enabled = true --set eventstreams.apikeyConfigMap = es-eventstreams-apikey --set postgresql.capemRequired = true --set postgresql.capemSecret = postgresql-ca-pem --set postgresql.urlSecret = postgresql-url --set postgresql.userSecret = postgresql-user --set postgresql.passwordSecret = postgresql-pwd --set serviceAccountName = kcontainer-runtime --output-dir templates --namespace eda-refarch chart/springcontainerms Deploy application using oc apply : oc apply -f templates/springcontainerms/templates","title":"Deploy to OCP 3.11"},{"location":"flask/","text":"Python Flask implementation of the container inventory management In this chapter we will implement a python based container inventory management service using kafka and flask. The component can be represented in the figure below: The container topics includes all events about container life cycle. Each event published to the container topic will have a corresponding action that will update the container's status. Container Actions ContainerAddedToInventory ContainerRemovedFromInventory ContainerAtLocation ContainerOnMaintenance ContainerOffMaintenance ContainerAssignedToOrder ContainerReleasedFromOrder ContainerGoodLoaded ContainerGoodUnLoaded ContainerOnShip ContainerOffShip ContainerOnTruck ContainerOffTruck Container States OnShip OffShip AtDock OnTruck OffTruck Loaded Empty","title":"Python Flask and Kafka Implementation"},{"location":"flask/#python-flask-implementation-of-the-container-inventory-management","text":"In this chapter we will implement a python based container inventory management service using kafka and flask. The component can be represented in the figure below: The container topics includes all events about container life cycle. Each event published to the container topic will have a corresponding action that will update the container's status.","title":"Python Flask implementation of the container inventory management"},{"location":"flask/#container-actions","text":"ContainerAddedToInventory ContainerRemovedFromInventory ContainerAtLocation ContainerOnMaintenance ContainerOffMaintenance ContainerAssignedToOrder ContainerReleasedFromOrder ContainerGoodLoaded ContainerGoodUnLoaded ContainerOnShip ContainerOffShip ContainerOnTruck ContainerOffTruck","title":"Container Actions"},{"location":"flask/#container-states","text":"OnShip OffShip AtDock OnTruck OffTruck Loaded Empty","title":"Container States"},{"location":"kstreams/","text":"Kafka Streams implementation of the container inventory management In this chapter we are presenting how to sue the Kafka Streams API combined with Kafka event sourcing to implement the container inventory management. The component can be represented in the figure below: For getting started with Kafka Streams API read this tutorial . The container topics includes all the event about container life cycle. The application is Java based and deployed in Liberty packaged into a docker image deployable on Kubernetes. The service exposes some RESTful APIs to get a container by ID. No CUD operations as all is done via events. The Streams implementation keeps data in Ktable. As a java based microservice we have two approaches to implement the service: springboot and microprofile. Knowing we will deploy on kubernetes cluster with Istio we will have a lot of the resiliency and scalability addressed for us. Microprofile add a lot of nice capabilities like SSL, open API, JAXRS... Microprofile is supported by Open Liberty as well as many servers. The Apache Kafka Streams API is a client library for building applications and microservices, where the input and output data are stored in Kafka clusters. It simplifies the implementation of the stateless or stateful event processing to transform and enrich data. It supports time windowing processing. We encourage to do this Streams tutorial . The features we want to illustrate in this implementation, using KStreams are: Listen to ContainerAddedToInventory event from the containers topic and maintains a stateful table of containers. Listen to OrderCreated event from orders and assign a container from the inventory based on the pickup location and the container location and its characteristics. Implemented as JAXRS application deployed on Liberty and packaged with dockerfile. Deploy to kubernetes or run with docker-compose Start with maven Kafka streams delivers a Maven archetype to create a squeleton project. The following command can be used to create the base code. mvn archetype:generate -DarchetypeGroupId = org.apache.kafka -DarchetypeArtifactId = streams-quickstart-java -DarchetypeVersion = 2 .1.0 -DgroupId = kc-container -DartifactId = kc-container-streams -Dversion = 0 .1 -Dpackage = containerManager We added a .project file to develop the code in Eclipse, imported the code into Eclipse and modify the .classpath with the following lines: <classpathentry kind= \"con\" path= \"org.eclipse.m2e.MAVEN2_CLASSPATH_CONTAINER\" > <attributes> <attribute name= \"maven.pomderived\" value= \"true\" /> </attributes> </classpathentry> To access to serializer and testing framework we added the following dependencies in the pom.xml: <dependency> <groupId> org.apache.kafka </groupId> <artifactId> kafka-clients </artifactId> <version> ${kafka.version} </version> </dependency> <dependency> <groupId> org.apache.kafka </groupId> <artifactId> kafka-streams-test-utils </artifactId> <version> ${kafka.version} </version> <scope> test </scope> </dependency> Using this approach as the service runs in OpenLiberty and integrate JAXRS, microprofile, Open API,... we have to add a lot of depencies into the pom.xml file. Another approach is to use IBM Cloud starter kit. Start with IBM Cloud microprofile starter kit Use the Create resource option and select the \"Java microservice with microprofile and Java EE\" starter kit as shown below: Then enter an application name (e.g. MP-ContainerMS) with a resource group and may be some tags. The next step is to select a kubernetes cluster instance: Configure the toolchain, and verify the application is created in the console: The application is accessible from github, a toolchain is ready to process the app and deploy it. At the time of writting, and most likely in the future too, the steps and the documentations are not aligned. Code is release on a weekly basis and the documentation is often behind. We can download the source code from the github. The address was https://git.ng.bluemix.net/boyerje/MP-ContainerMS. We have to unprotect the master branch so we can push our update. We also have to modify the deployment configuration to change the target namespace. The generated code includes helm chart, Dockerfiles, and base JAXRS code. The code generated is very similar to the one created using the ibmcloud dev CLI. But we need to modify this generated project with some specific microprofile content. Eclipse microprofile is now on version 2.2, so we also use the following code generator from the Microprofile starter site so we can get updated code with new capability like SSL, openAPI and JWT supports. So now we need to integrate both code and then add Kafka streams. Here are some of the main updates we did: Add in the cli-config.yml the registry address and cluster name Change pom dependencies for microprofile 2.2, and change the image in Dockerfile to access websphere liberty 19.0.0.3 compatible with 2.2. (FROM websphere-liberty:19.0.0.3-microProfile2) Use the health class from the microprofile 2.2 generated code, as it uses microprofile annotation. Add also the configuration injection with properties file. Add new package names. Remove unnecessary files * Modify the Values.yaml to reflect the target registry and add secret reference: repository: us.icr.io/ibmcaseeda/mpcontainerms tag: latest pullPolicy: Always pullSecret: browncompute-registry-secret Some of those steps are pushed to the development kubernetes cluster using the command: Some useful Kafka streams APIs The stream configuration looks similar to the Kafka consumer and producer configuration. props . put ( StreamsConfig . APPLICATION_ID_CONFIG , \"container-streams\" ); props . put ( StreamsConfig . BOOTSTRAP_SERVERS_CONFIG , \"localhost:9092\" ); props . put ( StreamsConfig . DEFAULT_KEY_SERDE_CLASS_CONFIG , Serdes . String (). getClass ()); props . put ( StreamsConfig . DEFAULT_VALUE_SERDE_CLASS_CONFIG , Serdes . String (). getClass ()); The StreamsConfig is a specific configuration for Streams app. One of the interesting class is the KStream to manage a stream of structured events. Kstreams represents unbounded collection of immutable events. Two classes are supporting the order and container processing: ContainerInventoryView ContainerOrderAssignment We are using the Streams DSL APIs to do the processing. Here is an example of terminal stream to print what is coming to the orders topic: final StreamsBuilder builder = new StreamsBuilder (); builder . stream ( \"orders\" ) . foreach (( key , value ) -> { Order order = parser . fromJson (( String ) value , OrderEvent . class ). getPayload (); // TODO do something to the order System . out . println ( \"received order \" + key + \" \" + value ); }); final Topology topology = builder . build (); final KafkaStreams streams = new KafkaStreams ( topology , props ); streams . start (); We want now to implement the container inventory. We want to support the following events: ContainerAddedToInventory, ContainerRemovedFromInventory ContainerAtLocation ContainerOnMaintenance, ContainerOffMaintenance, ContainerAssignedToOrder, ContainerReleasedFromOrder ContainerGoodLoaded, ContainerGoodUnLoaded ContainerOnShip, ContainerOffShip ContainerOnTruck, ContainerOffTruck We want the container event to keep a timestamp, a version, a type, and a payload representing the data describing a Reefer container. The Key is the containerID. The java class for the container event is here . Using a TDD approach we will start by the tests to implement the solution. For more information on the Streams DSL API, keep this page close to you . Test Driven Development We want to document two major test suites. One for building the internal view of the container inventory, the other to support the container to order assignment. Container inventory When the service receives a ContainerAdded event it needs to add it to the table and be able to retreive it by ID To support the Get By ID we are adding a Service class with the operation exposed as RESTful resource using JAXRS annotations. We already described this approach in the fleetms project . To test a stream application without Kafka backbone there is a test utility available here . The settings are simple: get the properties, define the serialisation of the key and value of the event to get from kafka, define the stream process flow, named topology, send the input and get the output. The test TestContainerInventory is illustrating how to use the TopologyTestDriver . Properties props = ApplicationConfig . getStreamsProperties ( \"test\" ); props . put ( StreamsConfig . BOOTSTRAP_SERVERS_CONFIG , \"dummy:1234\" ); TopologyTestDriver testDriver = new TopologyTestDriver ( buildProcessFlow (), props ); ConsumerRecordFactory < String , String > factory = new ConsumerRecordFactory < String , String >( \"containers\" , new StringSerializer (), new StringSerializer ()); ConsumerRecord < byte [], byte []> record = factory . create ( \"containers\" , ce . getContainerID (), parser . toJson ( ce )); testDriver . pipeInput ( record ); We are using the String default serialization for the key and the ContainerEvent, and use Gson to serialize and deserialize the json. So the test is to prepare a ContainerEvent with type = \"ContainerAdded\" and then get the payload, persist it in the table and access to the table via the concept of store and validate the data. Below is the access to the store and compare the expected results KeyValueStore < String , String > store = testDriver . getKeyValueStore ( \"queryable-container-store\" ); String containerStrg = store . get ( ce . getContainerID ()); Assert . assertNotNull ( containerStrg ); Assert . assertTrue ( containerStrg . contains ( ce . getContainerID ())); Assert . assertTrue ( containerStrg . contains ( \"atDock\" )); Now the tricky part is in the Stream process flow. The idea is to process the ContainerEvent as streams (of String) and extract the payload (a Container), then generate the Container in a new stream, group by the key and then save to a table. We separate the code in a function so e can move it into the real application after. public Topology buildProcessFlow () { final StreamsBuilder builder = new StreamsBuilder (); // containerEvent is a string, map values help to change the type and data of the inpit values builder . stream ( CONTAINERS_TOPIC ). mapValues (( containerEvent ) -> { // the container payload is of interest to keep in table Container c = jsonParser . fromJson (( String ) containerEvent , ContainerEvent . class ). getPayload (); return jsonParser . toJson ( c ); }). groupByKey () // the keys are kept so we can group by key to prepare for the tabl . reduce (( container , container1 ) -> { System . out . println ( \"received container \" + container1 ); return container1 ; }, Materialized . as ( \"queryable-container-store\" )); return builder . build (); } The trick is to use the reduce() function that get the container and save it to the store that we can specify. The unit test runs successfully with the command: mvn -Dtest=TestContainerInventory test . This logic can be integrated in a View class. So we can refactor the test and add new class (see ContainerInventoryView class) to move the logic into the applciation. From a design point of view this class is a DAO. Now that we are not using the Testing tool, we need the real streams. In class ContainerInventoryView: private KafkaStreams streams ; // .. Properties props = ApplicationConfig . getStreamsProperties ( \"container-streams\" ); props . put ( ConsumerConfig . AUTO_OFFSET_RESET_CONFIG , \"earliest\" ); streams = new KafkaStreams ( buildProcessFlow (), props ); try { streams . cleanUp (); streams . start (); } catch ( Throwable e ) { System . exit ( 1 ); } As illustrated above, the streams API is a continuous running Thread, so it needs to be started only one time. We will address scaling separatly. So we isolate the DAO as a Singleton, and start it when the deployed application starts, via a ServletContextListener. public class EventLoop implements ServletContextListener { @Override public void contextInitialized ( ServletContextEvent sce ) { // Initialize the Container consumer ContainerInventoryView cView = ( ContainerInventoryView ) ContainerInventoryView . instance (); cView . start (); } Now we can add the getById operation, package as a war, deploy it to Liberty. Container to Order Assignment The business logic we want to implement is to get an order with the source pickup city, the type of product, the quantity and the expected pickup date, manage the internal list of containers and search for a container located close to the pickup city from the order. The test to validate this logic is under kstreams/src/test/java/ut/TestContainerAssignment . The story will not be completed if we do not talk about how th application get the order. As presented in the design and order command microservice implementation, when an order is created an event is generated to the orders topic. So we need to add another Streams processing and start the process flow in the context listener. Run tests Recall with maven we can run all the unit tests, one test and skip integration tests. # Test a unique test $ mvn -Dtest = TestContainerInventory test # Skip all tests mvn install -DskipTests # Skip integration test mvn install -DskipITs # Run everything mvn install To start the liberty server use the script: ./script/startLocalLiberty or mvn liberty:run-server docker compose configuration Replace existing springcontainerms declaration to the following containerkstreams : image : ibmcase/kc-containerkstreams:latest hostname : containerkstreams ports : - \"12080:9080\" environment : KAFKA_ENV : ${KAFKA_ENV} KAFKA_BROKERS : ${KAFKA_BROKERS} KAFKA_APIKEY : ${KAFKA_APIKEY} How streams flows are resilient? Specifying the replicas factor at the topic level, with a cluster of kafka brokers, combine with transactional event produce, ensure to do not lose messages. The producer client code has the list of all the brokers to contact in case of failure and will try to connect to any broker in the list. How to scale?","title":"Kafka Streams Implementation"},{"location":"kstreams/#kafka-streams-implementation-of-the-container-inventory-management","text":"In this chapter we are presenting how to sue the Kafka Streams API combined with Kafka event sourcing to implement the container inventory management. The component can be represented in the figure below: For getting started with Kafka Streams API read this tutorial . The container topics includes all the event about container life cycle. The application is Java based and deployed in Liberty packaged into a docker image deployable on Kubernetes. The service exposes some RESTful APIs to get a container by ID. No CUD operations as all is done via events. The Streams implementation keeps data in Ktable. As a java based microservice we have two approaches to implement the service: springboot and microprofile. Knowing we will deploy on kubernetes cluster with Istio we will have a lot of the resiliency and scalability addressed for us. Microprofile add a lot of nice capabilities like SSL, open API, JAXRS... Microprofile is supported by Open Liberty as well as many servers. The Apache Kafka Streams API is a client library for building applications and microservices, where the input and output data are stored in Kafka clusters. It simplifies the implementation of the stateless or stateful event processing to transform and enrich data. It supports time windowing processing. We encourage to do this Streams tutorial . The features we want to illustrate in this implementation, using KStreams are: Listen to ContainerAddedToInventory event from the containers topic and maintains a stateful table of containers. Listen to OrderCreated event from orders and assign a container from the inventory based on the pickup location and the container location and its characteristics. Implemented as JAXRS application deployed on Liberty and packaged with dockerfile. Deploy to kubernetes or run with docker-compose","title":"Kafka Streams implementation of the container inventory management"},{"location":"kstreams/#start-with-maven","text":"Kafka streams delivers a Maven archetype to create a squeleton project. The following command can be used to create the base code. mvn archetype:generate -DarchetypeGroupId = org.apache.kafka -DarchetypeArtifactId = streams-quickstart-java -DarchetypeVersion = 2 .1.0 -DgroupId = kc-container -DartifactId = kc-container-streams -Dversion = 0 .1 -Dpackage = containerManager We added a .project file to develop the code in Eclipse, imported the code into Eclipse and modify the .classpath with the following lines: <classpathentry kind= \"con\" path= \"org.eclipse.m2e.MAVEN2_CLASSPATH_CONTAINER\" > <attributes> <attribute name= \"maven.pomderived\" value= \"true\" /> </attributes> </classpathentry> To access to serializer and testing framework we added the following dependencies in the pom.xml: <dependency> <groupId> org.apache.kafka </groupId> <artifactId> kafka-clients </artifactId> <version> ${kafka.version} </version> </dependency> <dependency> <groupId> org.apache.kafka </groupId> <artifactId> kafka-streams-test-utils </artifactId> <version> ${kafka.version} </version> <scope> test </scope> </dependency> Using this approach as the service runs in OpenLiberty and integrate JAXRS, microprofile, Open API,... we have to add a lot of depencies into the pom.xml file. Another approach is to use IBM Cloud starter kit.","title":"Start with maven"},{"location":"kstreams/#start-with-ibm-cloud-microprofile-starter-kit","text":"Use the Create resource option and select the \"Java microservice with microprofile and Java EE\" starter kit as shown below: Then enter an application name (e.g. MP-ContainerMS) with a resource group and may be some tags. The next step is to select a kubernetes cluster instance: Configure the toolchain, and verify the application is created in the console: The application is accessible from github, a toolchain is ready to process the app and deploy it. At the time of writting, and most likely in the future too, the steps and the documentations are not aligned. Code is release on a weekly basis and the documentation is often behind. We can download the source code from the github. The address was https://git.ng.bluemix.net/boyerje/MP-ContainerMS. We have to unprotect the master branch so we can push our update. We also have to modify the deployment configuration to change the target namespace. The generated code includes helm chart, Dockerfiles, and base JAXRS code. The code generated is very similar to the one created using the ibmcloud dev CLI. But we need to modify this generated project with some specific microprofile content. Eclipse microprofile is now on version 2.2, so we also use the following code generator from the Microprofile starter site so we can get updated code with new capability like SSL, openAPI and JWT supports. So now we need to integrate both code and then add Kafka streams. Here are some of the main updates we did: Add in the cli-config.yml the registry address and cluster name Change pom dependencies for microprofile 2.2, and change the image in Dockerfile to access websphere liberty 19.0.0.3 compatible with 2.2. (FROM websphere-liberty:19.0.0.3-microProfile2) Use the health class from the microprofile 2.2 generated code, as it uses microprofile annotation. Add also the configuration injection with properties file. Add new package names. Remove unnecessary files * Modify the Values.yaml to reflect the target registry and add secret reference: repository: us.icr.io/ibmcaseeda/mpcontainerms tag: latest pullPolicy: Always pullSecret: browncompute-registry-secret Some of those steps are pushed to the development kubernetes cluster using the command:","title":"Start with IBM Cloud microprofile starter kit"},{"location":"kstreams/#some-useful-kafka-streams-apis","text":"The stream configuration looks similar to the Kafka consumer and producer configuration. props . put ( StreamsConfig . APPLICATION_ID_CONFIG , \"container-streams\" ); props . put ( StreamsConfig . BOOTSTRAP_SERVERS_CONFIG , \"localhost:9092\" ); props . put ( StreamsConfig . DEFAULT_KEY_SERDE_CLASS_CONFIG , Serdes . String (). getClass ()); props . put ( StreamsConfig . DEFAULT_VALUE_SERDE_CLASS_CONFIG , Serdes . String (). getClass ()); The StreamsConfig is a specific configuration for Streams app. One of the interesting class is the KStream to manage a stream of structured events. Kstreams represents unbounded collection of immutable events. Two classes are supporting the order and container processing: ContainerInventoryView ContainerOrderAssignment We are using the Streams DSL APIs to do the processing. Here is an example of terminal stream to print what is coming to the orders topic: final StreamsBuilder builder = new StreamsBuilder (); builder . stream ( \"orders\" ) . foreach (( key , value ) -> { Order order = parser . fromJson (( String ) value , OrderEvent . class ). getPayload (); // TODO do something to the order System . out . println ( \"received order \" + key + \" \" + value ); }); final Topology topology = builder . build (); final KafkaStreams streams = new KafkaStreams ( topology , props ); streams . start (); We want now to implement the container inventory. We want to support the following events: ContainerAddedToInventory, ContainerRemovedFromInventory ContainerAtLocation ContainerOnMaintenance, ContainerOffMaintenance, ContainerAssignedToOrder, ContainerReleasedFromOrder ContainerGoodLoaded, ContainerGoodUnLoaded ContainerOnShip, ContainerOffShip ContainerOnTruck, ContainerOffTruck We want the container event to keep a timestamp, a version, a type, and a payload representing the data describing a Reefer container. The Key is the containerID. The java class for the container event is here . Using a TDD approach we will start by the tests to implement the solution. For more information on the Streams DSL API, keep this page close to you .","title":"Some useful Kafka streams APIs"},{"location":"kstreams/#test-driven-development","text":"We want to document two major test suites. One for building the internal view of the container inventory, the other to support the container to order assignment.","title":"Test Driven Development"},{"location":"kstreams/#container-inventory","text":"When the service receives a ContainerAdded event it needs to add it to the table and be able to retreive it by ID To support the Get By ID we are adding a Service class with the operation exposed as RESTful resource using JAXRS annotations. We already described this approach in the fleetms project . To test a stream application without Kafka backbone there is a test utility available here . The settings are simple: get the properties, define the serialisation of the key and value of the event to get from kafka, define the stream process flow, named topology, send the input and get the output. The test TestContainerInventory is illustrating how to use the TopologyTestDriver . Properties props = ApplicationConfig . getStreamsProperties ( \"test\" ); props . put ( StreamsConfig . BOOTSTRAP_SERVERS_CONFIG , \"dummy:1234\" ); TopologyTestDriver testDriver = new TopologyTestDriver ( buildProcessFlow (), props ); ConsumerRecordFactory < String , String > factory = new ConsumerRecordFactory < String , String >( \"containers\" , new StringSerializer (), new StringSerializer ()); ConsumerRecord < byte [], byte []> record = factory . create ( \"containers\" , ce . getContainerID (), parser . toJson ( ce )); testDriver . pipeInput ( record ); We are using the String default serialization for the key and the ContainerEvent, and use Gson to serialize and deserialize the json. So the test is to prepare a ContainerEvent with type = \"ContainerAdded\" and then get the payload, persist it in the table and access to the table via the concept of store and validate the data. Below is the access to the store and compare the expected results KeyValueStore < String , String > store = testDriver . getKeyValueStore ( \"queryable-container-store\" ); String containerStrg = store . get ( ce . getContainerID ()); Assert . assertNotNull ( containerStrg ); Assert . assertTrue ( containerStrg . contains ( ce . getContainerID ())); Assert . assertTrue ( containerStrg . contains ( \"atDock\" )); Now the tricky part is in the Stream process flow. The idea is to process the ContainerEvent as streams (of String) and extract the payload (a Container), then generate the Container in a new stream, group by the key and then save to a table. We separate the code in a function so e can move it into the real application after. public Topology buildProcessFlow () { final StreamsBuilder builder = new StreamsBuilder (); // containerEvent is a string, map values help to change the type and data of the inpit values builder . stream ( CONTAINERS_TOPIC ). mapValues (( containerEvent ) -> { // the container payload is of interest to keep in table Container c = jsonParser . fromJson (( String ) containerEvent , ContainerEvent . class ). getPayload (); return jsonParser . toJson ( c ); }). groupByKey () // the keys are kept so we can group by key to prepare for the tabl . reduce (( container , container1 ) -> { System . out . println ( \"received container \" + container1 ); return container1 ; }, Materialized . as ( \"queryable-container-store\" )); return builder . build (); } The trick is to use the reduce() function that get the container and save it to the store that we can specify. The unit test runs successfully with the command: mvn -Dtest=TestContainerInventory test . This logic can be integrated in a View class. So we can refactor the test and add new class (see ContainerInventoryView class) to move the logic into the applciation. From a design point of view this class is a DAO. Now that we are not using the Testing tool, we need the real streams. In class ContainerInventoryView: private KafkaStreams streams ; // .. Properties props = ApplicationConfig . getStreamsProperties ( \"container-streams\" ); props . put ( ConsumerConfig . AUTO_OFFSET_RESET_CONFIG , \"earliest\" ); streams = new KafkaStreams ( buildProcessFlow (), props ); try { streams . cleanUp (); streams . start (); } catch ( Throwable e ) { System . exit ( 1 ); } As illustrated above, the streams API is a continuous running Thread, so it needs to be started only one time. We will address scaling separatly. So we isolate the DAO as a Singleton, and start it when the deployed application starts, via a ServletContextListener. public class EventLoop implements ServletContextListener { @Override public void contextInitialized ( ServletContextEvent sce ) { // Initialize the Container consumer ContainerInventoryView cView = ( ContainerInventoryView ) ContainerInventoryView . instance (); cView . start (); } Now we can add the getById operation, package as a war, deploy it to Liberty.","title":"Container inventory"},{"location":"kstreams/#container-to-order-assignment","text":"The business logic we want to implement is to get an order with the source pickup city, the type of product, the quantity and the expected pickup date, manage the internal list of containers and search for a container located close to the pickup city from the order. The test to validate this logic is under kstreams/src/test/java/ut/TestContainerAssignment . The story will not be completed if we do not talk about how th application get the order. As presented in the design and order command microservice implementation, when an order is created an event is generated to the orders topic. So we need to add another Streams processing and start the process flow in the context listener.","title":"Container to Order Assignment"},{"location":"kstreams/#run-tests","text":"Recall with maven we can run all the unit tests, one test and skip integration tests. # Test a unique test $ mvn -Dtest = TestContainerInventory test # Skip all tests mvn install -DskipTests # Skip integration test mvn install -DskipITs # Run everything mvn install To start the liberty server use the script: ./script/startLocalLiberty or mvn liberty:run-server","title":"Run tests"},{"location":"kstreams/#docker-compose-configuration","text":"Replace existing springcontainerms declaration to the following containerkstreams : image : ibmcase/kc-containerkstreams:latest hostname : containerkstreams ports : - \"12080:9080\" environment : KAFKA_ENV : ${KAFKA_ENV} KAFKA_BROKERS : ${KAFKA_BROKERS} KAFKA_APIKEY : ${KAFKA_APIKEY}","title":"docker compose configuration"},{"location":"kstreams/#how-streams-flows-are-resilient","text":"Specifying the replicas factor at the topic level, with a cluster of kafka brokers, combine with transactional event produce, ensure to do not lose messages. The producer client code has the list of all the brokers to contact in case of failure and will try to connect to any broker in the list.","title":"How streams flows are resilient?"},{"location":"kstreams/#how-to-scale","text":"","title":"How to scale?"},{"location":"springboot/","text":"Springboot - Kafka container microservice This chapter presents how to develop the Reefer container manager service using Spring boot, Kafka template and PostgreSQL, Hibernate JPA and Spring data template. This is another way to implement the same specifications and helps us to assess the different technologies to develop event-driven cloud native microservice. The user stories to support are described in this section . The application is built around the following components: A Postgresql repository to support CRUD operations for Reefer containers A kafka consumer for container events, to get the container data and call the repository to save new container or update existing one. A new Order consumer to get order created event, and then search matching container(s) taking into account the product type, the quantity and the pickup address. It uses the postgresql repository to find the list of Reefer candidates. An order producer to send containerId - Order ID assignment event to the orders topic using order ID as key A container producer to send containerId - Order ID assignment event to the container topic, using container ID as key. Expose RESTful APIs to be a reusable service too. We are addressing one of the characteristics of microservice 'reversibility' with this project: the programming style and deployment to different cloud platform. We try to answer the following questions: How to deploy to IBM Cloud kubernetes service and in one command, taking the same code version and deploying it to IBM Cloud Private behind the firewall with all the related components available on that plaform? How to adopt control the dependencies and the configuration factors (See The 12 factors app ) to facilitate reversibility? Starting code We have found it is necessary to combine different starter code as the basic spring boot generated code is not useful. Start from Spring initializer Using the Spring Code generator we created the base project. The generated code is really minimum and the interesting part is the maven pom.xml dependencies. We do not have code related to the selected dependant, no dockerfile and all the goodies to deploy to Kubernetes cluster. It is more convenient to start from IBM Cloud starter kit and modify the generared pom.xml with the needed dependencies... Start from IBM Cloud Starter Kit. Once logged to IBM Cloud, create a resource and select Starter kit in the Catalog, then the Java microservice with Spring kit. The application is created, we can download the code or create a toolchain so we can use a continuous integration and deployment to an existing IBM Kubernetes Service. Now the code repository has Dockerfile, helm chart, CLI configuration, scripts and other manifests... We can build existing code and run it with the following commands: $ mvn clean package $ java -jar target/SpringContainerMS-1.0-SNAPSHOT.jar application.SBApplication Pointing to http://localhost:8080/ will bring the first page. It works! So let break it. If you did not try it before, and if you are using Eclipse last release, you can install Eclipse Spring IDE plugin: open Eclipse Marketplace and search for Spring IDE, and then download release 4.x: To verify the installation, importing our project as a 'maven' project will let you see the different options like spring boot starters election, etc... The tool is also available for IDE visual studio code. See this note for Spring toools. Update pom.xml The generated dependencies include Spring boot starter web code, hystrix for circuit breaker and retries, and testing. We need to add kafka and postgreSQL dependencies and re-run mvn install . <dependency> <groupId> org.springframework.kafka </groupId> <artifactId> spring-kafka </artifactId> </dependency> <dependency> <groupId> org.postgresql </groupId> <artifactId> postgresql </artifactId> <scope> runtime </scope> </dependency> <dependency> <groupId> org.springframework.kafka </groupId> <artifactId> spring-kafka-test </artifactId> <scope> test </scope> </dependency> Build Build with maven cd SpringContainerMS mvn package Build with docker To support flexible CI/CD deployment and run locally we propose to use Docker multi-stage build . As part of the CI/CD design, there is an important subject to address: how to support integration tests? Unit tests focus on validating the business logic, while integration tests validate the end to end integration of the microservice with its dependants services and products. We separated the unit tests and integration tests in their own Java packages. Later it will be more appropriate to have a separate code repository for integration tests (we stated this approach in the refarch-kc project under the itg-tests folder). Integration tests in this project access remote postegresql server. When the server is in IBM Cloud as part of the postgresql service, a SSL Certificate is defined as part of the service credentials. When building with Docker multi-stage this means the build stage has to get certificate in the Java TrustStore so tests are successful. We burnt a lot of time to make it working. We recommend reading the security section below to see what commands to run to get certificate in good format and create a truststore. Those commands have to be done in the dockerfile too and certificate, URL, user, password has to be injected using Dockerfile arguments and then environment variables. See the scripts/buildDocker.sh to assess the docker build parameters used. And then how to manage the certificate creation into the Java TrustStore using scripts like add_certificates.sh . This script needs to be executed before the maven build so any integration tests that need to access the remote Postgresql server will not fail with SSL handcheck process. And it needs to be done in the docker final image as part of the startup of the spring boot app (see startup.sh ). Some Spring boot basis We encourage you to go over the different tutorials from spring.io. We followed the following ones to be sure we have some good understanding of the basic components: Springboot . Helps to start quickly application. The first class as the main function and run the SpringApplcation. The @SpringBootApplication annotation adds configuration to get the spring bean definitions, and auto configure the bean jars needed. As we specified the pring-boot-starter-web artifact in the dependencoes the application is a web MVC and includes a Dispatcher servlet. Data with JPA addresses creating entity beans, repository as data access object and queries. Spring Boot, PostgreSQL, JPA, Hibernate RESTful CRUD API Example from Callicoder Spring Kafka Code implementation Add the get \"/containers\" API We want to start by the get reefer containers test and then implement a simple container controller bean with the CRUD operations. Spring takes into account the package names to manage its dependencies and settings. The test looks like below: @Test public void testGetContainers () throws Exception { String endpoint = \"http://localhost:\" + port + \"/containers\" ; String response = server . getForObject ( endpoint , String . class ); assertTrue ( \"Invalid response from server : \" + response , response . startsWith ( \"[\" )); } Spring Boot provides a @SpringBootTest annotation to build an application context used for the test. Below we specify to use Spring unit test runner, and to load the configuration from the main application class. The server used above is auto injected by the runner. @RunWith ( SpringRunner . class ) @SpringBootTest ( classes = SBApplication . class , webEnvironment = WebEnvironment . RANDOM_PORT ) public class ContainerAPITest { @Autowired private TestRestTemplate server ; @LocalServerPort private int port ; The next step is to implement the controller class under the package ibm.labs.kc.containermgr.rest.containers . package ibm.labs.kc.containermgr.rest.containers ; @RestController public class ContainerController { @GetMapping ( \"containers\" ) public List < ContainerEntity > getAllContainers () { return new ArrayList < ContainerEntity >; } The ContainerEntity is a class with JPA annotations, used to control the object to table mapping. Below is a simple entity definition : @Entity @Table ( name = \"containers\" ) public class ContainerEntity implements java . io . Serializable { @Id protected String id ; protected String type ; protected String status ; protected String brand ; } Once we have defined the entity attributes and how it is mapped to the table column, the next major step is to add a repository class: ContainerRepository.java and configure the application to use Postgress dialect so JPA can generate compatible DDLs. The repository is scrary easy, it just extends a base JpaRepository and specifies the primary key used as ID and the type of record as ContainerEntity. @Repository public interface ContainerRepository extends JpaRepository < ContainerEntity , String >{} Spring makes it magic to add save, findById, findAll... operations transparently. So now we can @Autowrite this repository into the controller class and add the integration tests. The test is in the Junit test class: PostgreSqlTest . In fact, for test reason we want to inject Mockup class in unit test via the controller contructor. @Test public void testAccessToRemoteDBSpringDatasource () { Optional < ContainerEntity > cOut = containerRepo . findById ( \"c1\" ); if (! cOut . isPresent ()) { ContainerEntity c = new ContainerEntity ( \"c1\" , \"Brand\" , \"Reefer\" , 100 , 0 , 0 ); c . setCreatedAt ( new Date ()); c . setUpdatedAt ( c . getCreatedAt ()); containerRepo . save ( c ); // add asserts... } } Finally the controller is integrating the repository: @Autowired private ContainerRepository containerRepository ; @GetMapping ( \"/containers/{containerId}\" ) public ContainerEntity getContainerById ( @PathVariable String containerId ) { return containerRepository . findById ( containerId ) . orElseThrow (() -> new ResourceNotFoundException ( \"Container not found with id \" + containerId )); } The test fails as it is missing the configuration to the postsgresql service. As we do not want to hardcode the password and URL end point or share secret in public github we define the configuration using environment variables. See the application.properties file. We have two choices for the Postgresql service: one using IBM Cloud and one running local docker image. To export the different environment variables we have a setenv.sh script (which we have defined a template for, named setenv.sh.tmpl in the main repository refarch-kc ) with one argument used to control the different environment varaibles for Kafka and Postgresql. The script argument should be one of LOCAL (default if no argument is given), MINIKUBE, IBMCLOUD, or ICP. To execute against the remote postgresql and Kafka brokers use: # Under the SpringContainerMS folder $ source ../../refarch-kc/scripts/setenv.sh IBMCLOUD $ mvn test To execute against a local kafka - postesgresql docker images, first you need to start the backend services: In the refarch-kc project, under the docker folder use the docker compose file:backbone-compose.yml (See this note for detail ): and then # Under the SpringContainerMS folder $ source ../../refarch-kc/scripts/setenv.sh LOCAL $ mvn test After the tests run successfully with a valid connection to IBM cloud, launching the spring boot app location and going to http://localhost:8080/containers/c1 will give you the data for the Container \"c1\". Preparing your local test environment We propose to use the local deployment to do unit tests and integration tests. The integration test for this application is quite complex and we propose to use a diagram to illustrate the components involved and how to start them. First be sure your back end services run: It will start zookeeper, kafka and postgresql. You need the refarch-kc main project for that which normally can be in one folder above this current project. To start the backend: $ cd refarch-kc $ ./scripts/local/launch.sh BACKEND Starting docker_zookeeper1_1 ... done Creating docker_postgres-db_1 ... done Starting docker_kafka1_1 ... done if you need to restart from empty topics, delete the kafka1 and zookeeper1 folders under refarch-kc/docker and restart the backend services. The integration tests To perform the integration test step by step do the following: ID Description Run 1 Springboot app which includes kafka consumers and producers, REST API and postgresql DAO ./scripts/run.sh 2 psql to access postgresql DB From the refarch-kc-container-ms folder do ./scripts/start_psql 3 containers consumer. To trace the containers topics. This code is in refarch-kc project in itg-tests/ContainersPython folder. Use a Terminal console: refarch-kc/itg-tests/ContainersPython/ runContainerConsumer.sh LOCAL c01 4 container events producer. To generate events. This code is in refarch-kc project in itg-tests/ContainersPython folder. Use a Terminal console: refarch-kc/itg-tests/ContainersPython/ addContainer.sh LOCAL c01 5 orders consumer. To trace the orders topics. This code is in refarch-kc project in itg-tests/OrdersPython folder. Use a Terminal console: refarch-kc/itg-tests/OrdersPython/ runOrderConsumer.sh LOCAL order01 6 orders events producer. To generate order events. This code is in refarch-kc project in itg-tests/OrdersPython folder. Use a Terminal console: refarch-kc/itg-tests/OrdersPython/ addOrder.sh LOCAL order01 Here is an example of traces after sending 2 container creation events in terminal console (#3): @@@ pollNextOrder containers partition: [0] at offset 4 with key b'itg-C02': value: {\"containerID\": \"itg-C02\", \"timestamp\": 1556250004, \"type\": \"ContainerAdded\", \"payload\": {\"containerID\": \"itg-C02\", \"type\": \"Reefer\", \"status\": \"Empty\", \"latitude\": 37.8, \"longitude\": -122.25, \"capacity\": 110, \"brand\": \"itg-brand\"}} @@@ pollNextOrder containers partition: [0] at offset 5 with key b'itg-c03': value: {\"containerID\": \"itg-c03\", \"timestamp\": 1556250673, \"type\": \"ContainerAdded\", \"payload\": {\"containerID\": \"itg-c03\", \"type\": \"Reefer\", \"status\": \"Empty\", \"latitude\": 37.8, \"longitude\": -122.25, \"capacity\": 110, \"brand\": \"itg-brand\"}} Which can be verified in the (#1) trace: 2019-04-25 16:52:11.245 INFO 17734 --- [ingConsumer-C-1] c.l.k.c.kafka.ContainerConsumer : Received container event: {\"containerID\": \"itg-C02\", \"timestamp\": 1556250004, \"type\": \"ContainerAdded\", \"payload\": {\"containerID\": \"itg-C02\", \"type\": \"Reefer\", \"status\": \"Empty\", \"latitude\": 37.8, \"longitude\": -122.25, \"capacity\": 110, \"brand\": \"itg-brand\"}} And in console for container producer (#4) Create container {'containerID': 'itg-C02', 'timestamp': 1556250004, 'type': 'ContainerAdded', 'payload': {'containerID': 'itg-C02', 'type': 'Reefer', 'status': 'Empty', 'latitude': 37.8, 'longitude': -122.25, 'capacity': 110, 'brand': 'itg-brand'}} Message delivered to containers [0] Finally in the psql console: postgres=# SELECT * FROM containers; id | brand | capacity | created_at | current_city | latitude | longitude | status | type | updated_at ---------+-----------+----------+-------------------------+--------------+----------+-----------+--------+--------+------------------------- itg-C02 | itg-brand | 110 | 2019-04-25 16:28:26.885 | Oakland | 37.8 | -122.25 | 1 | Reefer | 2019-04-25 20:40:04.579 itg-c03 | itg-brand | 110 | 2019-04-25 20:51:13.424 | Oakland | 37.8 | -122.25 | 1 | Reefer | 2019-04-25 20:51:13.424 The capacity is at maximum and the status = 1 is for empty container. When an order is created in console #6, a matching container is found and 2 events are created to present the container id and order id in both topics: containers and orders: Console 6: Create order $ ./addOrder.sh LOCAL order02 Create order { 'orderID' : 'order04' , 'timestamp' : 1556252841 , 'type' : 'OrderCreated' , 'payload' : { 'orderID' : 'order04' , 'productID' : 'FreshFoodItg' , 'customerID' : 'Customer007' , 'quantity' : 180 , 'pickupAddress' : { 'street' : 'astreet' , 'city' : 'Oakland' , 'country' : 'USA' , 'state' : 'CA' , 'zipcode' : '95000' } , 'destinationAddress' : { 'street' : 'bstreet' , 'city' : 'Beijing' , 'country' : 'China' , 'state' : 'NE' , 'zipcode' : '09000' } , 'pickupDate' : '2019-05-25' , 'expectedDeliveryDate' : '2019-06-25' }} Message delivered to orders [ 0 ] Console 5: consumer orders value: {\"orderID\": \"order04\", \"timestamp\": 1556252841, \"type\": \"OrderCreated\", \"payload\": {\"orderID\": \"order04\", \"productID\": \"FreshFoodItg\", \"customerID\": \"Customer007\", \"quantity\": 180, \"pickupAddress\": {\"street\": \"astreet\", \"city\": \"Oakland\", \"country\": \"USA\", \"state\": \"CA\", \"zipcode\": \"95000\"}, \"destinationAddress\": {\"street\": \"bstreet\", \"city\": \"Beijing\", \"country\": \"China\", \"state\": \"NE\", \"zipcode\": \"09000\"}, \"pickupDate\": \"2019-05-25\", \"expectedDeliveryDate\": \"2019-06-25\"}} @@@ pollNextOrder orders partition: [0] at offset 5 with key b'order04': value: {\"orderID\":\"order04\",\"payload\":{\"containerID\":\"itg-c03\",\"orderID\":\"order04\"},\"timestamp\":1556252842194,\"type\":\"ContainerAllocated\"} Console 1: app 2019-04-25 21:27:21.765 INFO 21610 --- [ingConsumer-C-1] c.l.kc.containermgr.kafka.OrderConsumer : Received order event:{\"orderID\": \"order04\", \"timestamp\": 1556252841, \"type\": \"OrderCreated\", \"payload\": {\"orderID\": \"order04\", \"productID\": \"FreshFoodItg\", \"customerID\": \"Customer007\", \"quantity\": 180, \"pickupAddress\": {\"street\": \"astreet\", \"city\": \"Oakland\", \"country\": \"USA\", \"state\": \"CA\", \"zipcode\": \"95000\"}, \"destinationAddress\": {\"street\": \"bstreet\", \"city\": \"Beijing\", \"country\": \"China\", \"state\": \"NE\", \"zipcode\": \"09000\"}, \"pickupDate\": \"2019-05-25\", \"expectedDeliveryDate\": \"2019-06-25\"}} 2019-04-25 21:27:22.199 INFO 21610 --- [ingConsumer-C-1] c.l.k.c.kafka.OrderProducerImpl : Emit order event:{\"orderID\":\"order04\",\"payload\":{\"containerID\":\"itg-c03\",\"orderID\":\"order04\"},\"timestamp\":1556252842194,\"type\":\"ContainerAllocated\"} 2019-04-25 21:27:22.273 INFO 21610 --- [ingConsumer-C-1] c.l.k.c.kafka.ContainerProducerImpl : Emit container event:{\"containerID\":\"itg-c03\",\"payload\":{\"containerID\":\"itg-c03\",\"orderID\":\"order04\"},\"timestamp\":1556252842272,\"type\":\"ContainerAssignedToOrder\"} Console 3: Containers consumer @@@ pollNextOrder containers partition: [0] at offset 6 with key b'itg-c03': value: {\"containerID\":\"itg-c03\",\"payload\":{\"containerID\":\"itg-c03\",\"orderID\":\"order04\"},\"timestamp\":1556252842272,\"type\":\"ContainerAssignedToOrder\"} Console 2 The postgresql containers table is also modified as capacity of the container is reduced by the order quantity and the status set to loaded (value 0): postgres=# SELECT * FROM containers; id | brand | capacity | created_at | current_city | latitude | longitude | status | type | updated_at ---------+-----------+----------+-------------------------+--------------+----------+-----------+--------+--------+------------------------- itg-C02 | itg-brand | 0 | 2019-04-25 16:28:26.885 | Oakland | 37.8 | -122.25 | 0 | Reefer | 2019-04-25 21:24:57.821 itg-c03 | itg-brand | 0 | 2019-04-25 20:51:13.424 | Oakland | 37.8 | -122.25 | 0 | Reefer | 2019-04-25 21:27:22.159 Listening to container events The use story is to get the container event from the kafka containers topics and add new Reefer container data to the inventory, or update existing container. So we need to add consumer and publisher using Spring Kafka . Spring Kafka template is based on the pure java kafka-clients jar but provides the same encapsulation as Spring JMS template. Here is the product documentation . First we need to start a kafka consumer when the application starts. We want one unique instance, so a singleton. As presented in this article , we select to add a Component that is an application listener (Annotation @EventListener) so when the spring context is running, we can start consuming messages from Kafka. @Component public class ContainerConsumer { @EventListener public void onApplicationEvent ( ContextRefreshedEvent event ) { // prepare kafka consumer, add an event handler callback and start the consumer } The code is in the class: ibm.labs.kc.containermgr.kafka.ContainerConsumer.java . Spring Kafka container](https://docs.spring.io/spring-kafka/docs/2.2.4.RELEASE/reference/#receiving-messages) is not bringing that much value on top of Kafka Java API, and we prefer learning a unique API and keep programming with the Kafka Java API. We have added an end to end integration test to create container events and see the payload persisted in Postgresql. When running the server locally, you may want to leverage our docker compose file to start a local kafka broker. Listening to Order Event The approach is the same as above and the supporting class is: ibm.labs.kc.containermgr.kafka.OrderConsumer . Security To communicate with IBM Cloud PostgresSQl service the client needs to use SSL certificates... First to avoid sharing userid and password in github, we use environment variables for postgressql url, user and password. There is a setenv.tmpl.sh in the scripts folder to use with your own settings. Rename it setenv.sh . This file is ignored by git. The values are coming from the IBM Cloud postgresql service. Also to run unit test in eclipse you need to set those environment variables in the run configuration as illustrated in the figure below: Those variables are used the Spring boot application.configuration file, for example for the postgresql URL: spring.datasource.url= ${ POSTGRESQL_URL } When the Spring repository class establishes a TLS or SSL communication to the Postgresql service in IBM Cloud (or ICP), both client and server negotiate a stateful connection by using a handshaking procedure. During this procedure, the server usually sends back its identification in the form of a digital certificate. Java programs store certificates in a repository called Java KeyStore (JKS). To create the SSL connection you will need to make the server public certificate available to your Java client JVM. The certificate is persisted in a Trustore. So get the server public certificate: postgresql.crt , then convert it to a form Java understands. To download a text version of the base64 certificate as defined in the connection credentials of the IBM Cloud postgressql service use the following command: $ ibmcloud cdb deployment-cacert Green-DB-PostgreSQL # if you do not have the cloud database plugin does the following and rerun previous command: $ ibmcloud plugin install cloud-databases Save the certificate to a file (e.g. postgressql.crt). Then transform it to a Java format and save it to a keystore used by Java # transform $ openssl x509 -in postgressql.crt -out postgressql.crt.der -outform der # save in keystore $ keytool -keystore clienttruststore -alias postgresql -import -file postgressql.crt.der -storepass changeit Then adding the following setting will let the Java program accesses the certificates from the keystore. java -jar -Djavax.net.ssl.trustStore=clienttruststore -Djavax.net.ssl.trustStorePassword=changeit ./target/SpringContainerMS-1.0-SNAPSHOT.jar application.SBApplication We recommend reading this section of Postgresql product documentation, and this article from Baeldung on SSL handshake in Java. Now to make all this available in docker container we propose to let the previous two commands run within the Dockerfile during the build process. Manually deploy to IKS The steps are the same as other project and we are providing the same type of tools: login to IBM Cloud - Adapt to your region. ibmcloud login -a https://api.us-east.bluemix.net connect to the IKS cluster ibmcloud ks region-set us-east ibmcloud ks cluster-config fabio-wdc-07 Define the Kube config variable so all the kubectl and helm commands will contact the good cluster. export KUBECONFIG= ~/.bluemix/plugins/container-service/clusters/fabio-wdc-07/kube-config-wdc07-fabio-wdc-07.yml login to the private registry ibmcloud cr login build the application jar and docker image using our script ./script/buildDocker.sh IBMCLOUD Push the docker image to the private registry docker push us.icr.io/ibmcaseeda/kc-springcontainerms Deploy the Helm release ./scripts/deployHelm which is equivalent to the command: helm install $chart/ --name $kname --namespace $ns If for any reason you get a \"image can't be pulled\" error. Verify the image is in the private registry with the command: docker cr image-list Then if the image exists, verify there is a secret defined to keep the security key to access the registry in the same namespace as you are deploying your app, and this secret is referenced in the yaml file: \"imagePullSecrets\": [ { \"name\": \"browncompute-registry-secret\" } ], Perform integration tests from local to remote IBM Cloud deployment As presented in the section above , we have a set of tools that can create container, orders and verify the code is running well in its complete deployed environment. The same scripts can be run with the IBMCLOUD argument. So below is the list of command to perform on you laptop while the Spring boot app is deployed on IKS, using Event Streams and Postgresql on IBM Cloud: Start a container consumer to trace the execution: in the folder refarch-kc/itg-tests/ContainersPython . It listens to get a container with id: \"ic-c01\" ./runContainerConsumer.sh IBMCLOUD ic-c01 Run a producer of container events to add a new container into the inventory: in the same folder refarch-kc/itg-tests/ContainersPython ./addContainer.sh IBMCLOUD ic-c01 Start an order consumer, to trace the traffic: in the folder refarch-kc/itg-tests/OrdersPython ./runOrderConsumer.sh IBMCLOUD order01 Create an order from the same folder: refarch-kc/itg-tests/OrdersPython addOrder.sh IBMCLOUD order01 Going to the web URL of the container service you should see the container added. Encountered issues with some solution The level of abstraction in Spring is nice when doing basic things but can become a nightmare when doing real bigger application including different libraries. Also migrating or using the last version 2.xx, bring changes to existing code and tests. Below is a list of iisues we spent time on: When JPA started, it creates / updates database schema, and for example enforced to have an Id as int while it was as string. As a bypass we create the table before in postsgresql using psql tool. When starting the spring data, JPA will try to connect to the PostgresSQL and execute a set of validation, one of them could generate the following exception: Method org.postgresql.jdbc.PgConnection.createClob() is not yet implemented . The explanation for the solution is here. When running the SpringContainer inside docker-compose the following exception may occur org.postgresql.util.PSQLException: Connection to postgres:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections while running the same code with maven using the same local backend works. Testing endpoint /health did not work on Spring 2.1.4. This is due a new Actuator capabilities ( spring-boot-starter-actuator ) that adds endpoints to manage a webapp in production. This is a nice feature. So remove any hold Health api class and modify the url to /actuator/health . Be sure to read this note on actuator. Deployed in Kubernetes service, the pod has issue on postgress SSL handshake. (org.postgresql.util.PSQLException: SSL error: Received fatal alert: handshake_failure). SSL handshakes are a mechanism by which a client and server establish the trust and logistics required to secure their connection over the network. This problem may be linked to a SSL certificate not found or wrong encryption protocol. We need to be sure a Java Keystore is defined and includes the public certificate coming from the server. See security section above. When deploying to IKS, we did see an exception about container topics not visible . This problem is coming from the fact that we are using Event Stream simplest deployment, which is multi tenants. And even if our code reach the brokers, our api key was wrong so we could get to another instance which did not have our topics. Be sure the api key is well defined in the kubernetes secret. (see this note to know how to do it ) References Spring Code generator Spring Kafka and the spring kafka documentation sptring Data and JPA SSL and postgresql Runninf postgresql in docker","title":"Run spring boot app locally"},{"location":"springboot/#springboot-kafka-container-microservice","text":"This chapter presents how to develop the Reefer container manager service using Spring boot, Kafka template and PostgreSQL, Hibernate JPA and Spring data template. This is another way to implement the same specifications and helps us to assess the different technologies to develop event-driven cloud native microservice. The user stories to support are described in this section . The application is built around the following components: A Postgresql repository to support CRUD operations for Reefer containers A kafka consumer for container events, to get the container data and call the repository to save new container or update existing one. A new Order consumer to get order created event, and then search matching container(s) taking into account the product type, the quantity and the pickup address. It uses the postgresql repository to find the list of Reefer candidates. An order producer to send containerId - Order ID assignment event to the orders topic using order ID as key A container producer to send containerId - Order ID assignment event to the container topic, using container ID as key. Expose RESTful APIs to be a reusable service too. We are addressing one of the characteristics of microservice 'reversibility' with this project: the programming style and deployment to different cloud platform. We try to answer the following questions: How to deploy to IBM Cloud kubernetes service and in one command, taking the same code version and deploying it to IBM Cloud Private behind the firewall with all the related components available on that plaform? How to adopt control the dependencies and the configuration factors (See The 12 factors app ) to facilitate reversibility?","title":"Springboot - Kafka container microservice"},{"location":"springboot/#starting-code","text":"We have found it is necessary to combine different starter code as the basic spring boot generated code is not useful.","title":"Starting code"},{"location":"springboot/#start-from-spring-initializer","text":"Using the Spring Code generator we created the base project. The generated code is really minimum and the interesting part is the maven pom.xml dependencies. We do not have code related to the selected dependant, no dockerfile and all the goodies to deploy to Kubernetes cluster. It is more convenient to start from IBM Cloud starter kit and modify the generared pom.xml with the needed dependencies...","title":"Start from Spring initializer"},{"location":"springboot/#start-from-ibm-cloud-starter-kit","text":"Once logged to IBM Cloud, create a resource and select Starter kit in the Catalog, then the Java microservice with Spring kit. The application is created, we can download the code or create a toolchain so we can use a continuous integration and deployment to an existing IBM Kubernetes Service. Now the code repository has Dockerfile, helm chart, CLI configuration, scripts and other manifests... We can build existing code and run it with the following commands: $ mvn clean package $ java -jar target/SpringContainerMS-1.0-SNAPSHOT.jar application.SBApplication Pointing to http://localhost:8080/ will bring the first page. It works! So let break it. If you did not try it before, and if you are using Eclipse last release, you can install Eclipse Spring IDE plugin: open Eclipse Marketplace and search for Spring IDE, and then download release 4.x: To verify the installation, importing our project as a 'maven' project will let you see the different options like spring boot starters election, etc... The tool is also available for IDE visual studio code. See this note for Spring toools.","title":"Start from IBM Cloud Starter Kit."},{"location":"springboot/#update-pomxml","text":"The generated dependencies include Spring boot starter web code, hystrix for circuit breaker and retries, and testing. We need to add kafka and postgreSQL dependencies and re-run mvn install . <dependency> <groupId> org.springframework.kafka </groupId> <artifactId> spring-kafka </artifactId> </dependency> <dependency> <groupId> org.postgresql </groupId> <artifactId> postgresql </artifactId> <scope> runtime </scope> </dependency> <dependency> <groupId> org.springframework.kafka </groupId> <artifactId> spring-kafka-test </artifactId> <scope> test </scope> </dependency>","title":"Update pom.xml"},{"location":"springboot/#build","text":"","title":"Build"},{"location":"springboot/#build-with-maven","text":"cd SpringContainerMS mvn package","title":"Build with maven"},{"location":"springboot/#build-with-docker","text":"To support flexible CI/CD deployment and run locally we propose to use Docker multi-stage build . As part of the CI/CD design, there is an important subject to address: how to support integration tests? Unit tests focus on validating the business logic, while integration tests validate the end to end integration of the microservice with its dependants services and products. We separated the unit tests and integration tests in their own Java packages. Later it will be more appropriate to have a separate code repository for integration tests (we stated this approach in the refarch-kc project under the itg-tests folder). Integration tests in this project access remote postegresql server. When the server is in IBM Cloud as part of the postgresql service, a SSL Certificate is defined as part of the service credentials. When building with Docker multi-stage this means the build stage has to get certificate in the Java TrustStore so tests are successful. We burnt a lot of time to make it working. We recommend reading the security section below to see what commands to run to get certificate in good format and create a truststore. Those commands have to be done in the dockerfile too and certificate, URL, user, password has to be injected using Dockerfile arguments and then environment variables. See the scripts/buildDocker.sh to assess the docker build parameters used. And then how to manage the certificate creation into the Java TrustStore using scripts like add_certificates.sh . This script needs to be executed before the maven build so any integration tests that need to access the remote Postgresql server will not fail with SSL handcheck process. And it needs to be done in the docker final image as part of the startup of the spring boot app (see startup.sh ).","title":"Build with docker"},{"location":"springboot/#some-spring-boot-basis","text":"We encourage you to go over the different tutorials from spring.io. We followed the following ones to be sure we have some good understanding of the basic components: Springboot . Helps to start quickly application. The first class as the main function and run the SpringApplcation. The @SpringBootApplication annotation adds configuration to get the spring bean definitions, and auto configure the bean jars needed. As we specified the pring-boot-starter-web artifact in the dependencoes the application is a web MVC and includes a Dispatcher servlet. Data with JPA addresses creating entity beans, repository as data access object and queries. Spring Boot, PostgreSQL, JPA, Hibernate RESTful CRUD API Example from Callicoder Spring Kafka","title":"Some Spring boot basis"},{"location":"springboot/#code-implementation","text":"","title":"Code implementation"},{"location":"springboot/#add-the-get-containers-api","text":"We want to start by the get reefer containers test and then implement a simple container controller bean with the CRUD operations. Spring takes into account the package names to manage its dependencies and settings. The test looks like below: @Test public void testGetContainers () throws Exception { String endpoint = \"http://localhost:\" + port + \"/containers\" ; String response = server . getForObject ( endpoint , String . class ); assertTrue ( \"Invalid response from server : \" + response , response . startsWith ( \"[\" )); } Spring Boot provides a @SpringBootTest annotation to build an application context used for the test. Below we specify to use Spring unit test runner, and to load the configuration from the main application class. The server used above is auto injected by the runner. @RunWith ( SpringRunner . class ) @SpringBootTest ( classes = SBApplication . class , webEnvironment = WebEnvironment . RANDOM_PORT ) public class ContainerAPITest { @Autowired private TestRestTemplate server ; @LocalServerPort private int port ; The next step is to implement the controller class under the package ibm.labs.kc.containermgr.rest.containers . package ibm.labs.kc.containermgr.rest.containers ; @RestController public class ContainerController { @GetMapping ( \"containers\" ) public List < ContainerEntity > getAllContainers () { return new ArrayList < ContainerEntity >; } The ContainerEntity is a class with JPA annotations, used to control the object to table mapping. Below is a simple entity definition : @Entity @Table ( name = \"containers\" ) public class ContainerEntity implements java . io . Serializable { @Id protected String id ; protected String type ; protected String status ; protected String brand ; } Once we have defined the entity attributes and how it is mapped to the table column, the next major step is to add a repository class: ContainerRepository.java and configure the application to use Postgress dialect so JPA can generate compatible DDLs. The repository is scrary easy, it just extends a base JpaRepository and specifies the primary key used as ID and the type of record as ContainerEntity. @Repository public interface ContainerRepository extends JpaRepository < ContainerEntity , String >{} Spring makes it magic to add save, findById, findAll... operations transparently. So now we can @Autowrite this repository into the controller class and add the integration tests. The test is in the Junit test class: PostgreSqlTest . In fact, for test reason we want to inject Mockup class in unit test via the controller contructor. @Test public void testAccessToRemoteDBSpringDatasource () { Optional < ContainerEntity > cOut = containerRepo . findById ( \"c1\" ); if (! cOut . isPresent ()) { ContainerEntity c = new ContainerEntity ( \"c1\" , \"Brand\" , \"Reefer\" , 100 , 0 , 0 ); c . setCreatedAt ( new Date ()); c . setUpdatedAt ( c . getCreatedAt ()); containerRepo . save ( c ); // add asserts... } } Finally the controller is integrating the repository: @Autowired private ContainerRepository containerRepository ; @GetMapping ( \"/containers/{containerId}\" ) public ContainerEntity getContainerById ( @PathVariable String containerId ) { return containerRepository . findById ( containerId ) . orElseThrow (() -> new ResourceNotFoundException ( \"Container not found with id \" + containerId )); } The test fails as it is missing the configuration to the postsgresql service. As we do not want to hardcode the password and URL end point or share secret in public github we define the configuration using environment variables. See the application.properties file. We have two choices for the Postgresql service: one using IBM Cloud and one running local docker image. To export the different environment variables we have a setenv.sh script (which we have defined a template for, named setenv.sh.tmpl in the main repository refarch-kc ) with one argument used to control the different environment varaibles for Kafka and Postgresql. The script argument should be one of LOCAL (default if no argument is given), MINIKUBE, IBMCLOUD, or ICP. To execute against the remote postgresql and Kafka brokers use: # Under the SpringContainerMS folder $ source ../../refarch-kc/scripts/setenv.sh IBMCLOUD $ mvn test To execute against a local kafka - postesgresql docker images, first you need to start the backend services: In the refarch-kc project, under the docker folder use the docker compose file:backbone-compose.yml (See this note for detail ): and then # Under the SpringContainerMS folder $ source ../../refarch-kc/scripts/setenv.sh LOCAL $ mvn test After the tests run successfully with a valid connection to IBM cloud, launching the spring boot app location and going to http://localhost:8080/containers/c1 will give you the data for the Container \"c1\".","title":"Add the get \"/containers\"  API"},{"location":"springboot/#preparing-your-local-test-environment","text":"We propose to use the local deployment to do unit tests and integration tests. The integration test for this application is quite complex and we propose to use a diagram to illustrate the components involved and how to start them. First be sure your back end services run: It will start zookeeper, kafka and postgresql. You need the refarch-kc main project for that which normally can be in one folder above this current project. To start the backend: $ cd refarch-kc $ ./scripts/local/launch.sh BACKEND Starting docker_zookeeper1_1 ... done Creating docker_postgres-db_1 ... done Starting docker_kafka1_1 ... done if you need to restart from empty topics, delete the kafka1 and zookeeper1 folders under refarch-kc/docker and restart the backend services.","title":"Preparing your local test environment"},{"location":"springboot/#the-integration-tests","text":"To perform the integration test step by step do the following: ID Description Run 1 Springboot app which includes kafka consumers and producers, REST API and postgresql DAO ./scripts/run.sh 2 psql to access postgresql DB From the refarch-kc-container-ms folder do ./scripts/start_psql 3 containers consumer. To trace the containers topics. This code is in refarch-kc project in itg-tests/ContainersPython folder. Use a Terminal console: refarch-kc/itg-tests/ContainersPython/ runContainerConsumer.sh LOCAL c01 4 container events producer. To generate events. This code is in refarch-kc project in itg-tests/ContainersPython folder. Use a Terminal console: refarch-kc/itg-tests/ContainersPython/ addContainer.sh LOCAL c01 5 orders consumer. To trace the orders topics. This code is in refarch-kc project in itg-tests/OrdersPython folder. Use a Terminal console: refarch-kc/itg-tests/OrdersPython/ runOrderConsumer.sh LOCAL order01 6 orders events producer. To generate order events. This code is in refarch-kc project in itg-tests/OrdersPython folder. Use a Terminal console: refarch-kc/itg-tests/OrdersPython/ addOrder.sh LOCAL order01 Here is an example of traces after sending 2 container creation events in terminal console (#3): @@@ pollNextOrder containers partition: [0] at offset 4 with key b'itg-C02': value: {\"containerID\": \"itg-C02\", \"timestamp\": 1556250004, \"type\": \"ContainerAdded\", \"payload\": {\"containerID\": \"itg-C02\", \"type\": \"Reefer\", \"status\": \"Empty\", \"latitude\": 37.8, \"longitude\": -122.25, \"capacity\": 110, \"brand\": \"itg-brand\"}} @@@ pollNextOrder containers partition: [0] at offset 5 with key b'itg-c03': value: {\"containerID\": \"itg-c03\", \"timestamp\": 1556250673, \"type\": \"ContainerAdded\", \"payload\": {\"containerID\": \"itg-c03\", \"type\": \"Reefer\", \"status\": \"Empty\", \"latitude\": 37.8, \"longitude\": -122.25, \"capacity\": 110, \"brand\": \"itg-brand\"}} Which can be verified in the (#1) trace: 2019-04-25 16:52:11.245 INFO 17734 --- [ingConsumer-C-1] c.l.k.c.kafka.ContainerConsumer : Received container event: {\"containerID\": \"itg-C02\", \"timestamp\": 1556250004, \"type\": \"ContainerAdded\", \"payload\": {\"containerID\": \"itg-C02\", \"type\": \"Reefer\", \"status\": \"Empty\", \"latitude\": 37.8, \"longitude\": -122.25, \"capacity\": 110, \"brand\": \"itg-brand\"}} And in console for container producer (#4) Create container {'containerID': 'itg-C02', 'timestamp': 1556250004, 'type': 'ContainerAdded', 'payload': {'containerID': 'itg-C02', 'type': 'Reefer', 'status': 'Empty', 'latitude': 37.8, 'longitude': -122.25, 'capacity': 110, 'brand': 'itg-brand'}} Message delivered to containers [0] Finally in the psql console: postgres=# SELECT * FROM containers; id | brand | capacity | created_at | current_city | latitude | longitude | status | type | updated_at ---------+-----------+----------+-------------------------+--------------+----------+-----------+--------+--------+------------------------- itg-C02 | itg-brand | 110 | 2019-04-25 16:28:26.885 | Oakland | 37.8 | -122.25 | 1 | Reefer | 2019-04-25 20:40:04.579 itg-c03 | itg-brand | 110 | 2019-04-25 20:51:13.424 | Oakland | 37.8 | -122.25 | 1 | Reefer | 2019-04-25 20:51:13.424 The capacity is at maximum and the status = 1 is for empty container. When an order is created in console #6, a matching container is found and 2 events are created to present the container id and order id in both topics: containers and orders:","title":"The integration tests"},{"location":"springboot/#console-6-create-order","text":"$ ./addOrder.sh LOCAL order02 Create order { 'orderID' : 'order04' , 'timestamp' : 1556252841 , 'type' : 'OrderCreated' , 'payload' : { 'orderID' : 'order04' , 'productID' : 'FreshFoodItg' , 'customerID' : 'Customer007' , 'quantity' : 180 , 'pickupAddress' : { 'street' : 'astreet' , 'city' : 'Oakland' , 'country' : 'USA' , 'state' : 'CA' , 'zipcode' : '95000' } , 'destinationAddress' : { 'street' : 'bstreet' , 'city' : 'Beijing' , 'country' : 'China' , 'state' : 'NE' , 'zipcode' : '09000' } , 'pickupDate' : '2019-05-25' , 'expectedDeliveryDate' : '2019-06-25' }} Message delivered to orders [ 0 ]","title":"Console 6: Create order"},{"location":"springboot/#console-5-consumer-orders","text":"value: {\"orderID\": \"order04\", \"timestamp\": 1556252841, \"type\": \"OrderCreated\", \"payload\": {\"orderID\": \"order04\", \"productID\": \"FreshFoodItg\", \"customerID\": \"Customer007\", \"quantity\": 180, \"pickupAddress\": {\"street\": \"astreet\", \"city\": \"Oakland\", \"country\": \"USA\", \"state\": \"CA\", \"zipcode\": \"95000\"}, \"destinationAddress\": {\"street\": \"bstreet\", \"city\": \"Beijing\", \"country\": \"China\", \"state\": \"NE\", \"zipcode\": \"09000\"}, \"pickupDate\": \"2019-05-25\", \"expectedDeliveryDate\": \"2019-06-25\"}} @@@ pollNextOrder orders partition: [0] at offset 5 with key b'order04': value: {\"orderID\":\"order04\",\"payload\":{\"containerID\":\"itg-c03\",\"orderID\":\"order04\"},\"timestamp\":1556252842194,\"type\":\"ContainerAllocated\"}","title":"Console 5: consumer orders"},{"location":"springboot/#console-1-app","text":"2019-04-25 21:27:21.765 INFO 21610 --- [ingConsumer-C-1] c.l.kc.containermgr.kafka.OrderConsumer : Received order event:{\"orderID\": \"order04\", \"timestamp\": 1556252841, \"type\": \"OrderCreated\", \"payload\": {\"orderID\": \"order04\", \"productID\": \"FreshFoodItg\", \"customerID\": \"Customer007\", \"quantity\": 180, \"pickupAddress\": {\"street\": \"astreet\", \"city\": \"Oakland\", \"country\": \"USA\", \"state\": \"CA\", \"zipcode\": \"95000\"}, \"destinationAddress\": {\"street\": \"bstreet\", \"city\": \"Beijing\", \"country\": \"China\", \"state\": \"NE\", \"zipcode\": \"09000\"}, \"pickupDate\": \"2019-05-25\", \"expectedDeliveryDate\": \"2019-06-25\"}} 2019-04-25 21:27:22.199 INFO 21610 --- [ingConsumer-C-1] c.l.k.c.kafka.OrderProducerImpl : Emit order event:{\"orderID\":\"order04\",\"payload\":{\"containerID\":\"itg-c03\",\"orderID\":\"order04\"},\"timestamp\":1556252842194,\"type\":\"ContainerAllocated\"} 2019-04-25 21:27:22.273 INFO 21610 --- [ingConsumer-C-1] c.l.k.c.kafka.ContainerProducerImpl : Emit container event:{\"containerID\":\"itg-c03\",\"payload\":{\"containerID\":\"itg-c03\",\"orderID\":\"order04\"},\"timestamp\":1556252842272,\"type\":\"ContainerAssignedToOrder\"}","title":"Console 1: app"},{"location":"springboot/#console-3-containers-consumer","text":"@@@ pollNextOrder containers partition: [0] at offset 6 with key b'itg-c03': value: {\"containerID\":\"itg-c03\",\"payload\":{\"containerID\":\"itg-c03\",\"orderID\":\"order04\"},\"timestamp\":1556252842272,\"type\":\"ContainerAssignedToOrder\"}","title":"Console 3: Containers consumer"},{"location":"springboot/#console-2","text":"The postgresql containers table is also modified as capacity of the container is reduced by the order quantity and the status set to loaded (value 0): postgres=# SELECT * FROM containers; id | brand | capacity | created_at | current_city | latitude | longitude | status | type | updated_at ---------+-----------+----------+-------------------------+--------------+----------+-----------+--------+--------+------------------------- itg-C02 | itg-brand | 0 | 2019-04-25 16:28:26.885 | Oakland | 37.8 | -122.25 | 0 | Reefer | 2019-04-25 21:24:57.821 itg-c03 | itg-brand | 0 | 2019-04-25 20:51:13.424 | Oakland | 37.8 | -122.25 | 0 | Reefer | 2019-04-25 21:27:22.159","title":"Console 2"},{"location":"springboot/#listening-to-container-events","text":"The use story is to get the container event from the kafka containers topics and add new Reefer container data to the inventory, or update existing container. So we need to add consumer and publisher using Spring Kafka . Spring Kafka template is based on the pure java kafka-clients jar but provides the same encapsulation as Spring JMS template. Here is the product documentation . First we need to start a kafka consumer when the application starts. We want one unique instance, so a singleton. As presented in this article , we select to add a Component that is an application listener (Annotation @EventListener) so when the spring context is running, we can start consuming messages from Kafka. @Component public class ContainerConsumer { @EventListener public void onApplicationEvent ( ContextRefreshedEvent event ) { // prepare kafka consumer, add an event handler callback and start the consumer } The code is in the class: ibm.labs.kc.containermgr.kafka.ContainerConsumer.java . Spring Kafka container](https://docs.spring.io/spring-kafka/docs/2.2.4.RELEASE/reference/#receiving-messages) is not bringing that much value on top of Kafka Java API, and we prefer learning a unique API and keep programming with the Kafka Java API. We have added an end to end integration test to create container events and see the payload persisted in Postgresql. When running the server locally, you may want to leverage our docker compose file to start a local kafka broker.","title":"Listening to container events"},{"location":"springboot/#listening-to-order-event","text":"The approach is the same as above and the supporting class is: ibm.labs.kc.containermgr.kafka.OrderConsumer .","title":"Listening to Order Event"},{"location":"springboot/#security","text":"To communicate with IBM Cloud PostgresSQl service the client needs to use SSL certificates... First to avoid sharing userid and password in github, we use environment variables for postgressql url, user and password. There is a setenv.tmpl.sh in the scripts folder to use with your own settings. Rename it setenv.sh . This file is ignored by git. The values are coming from the IBM Cloud postgresql service. Also to run unit test in eclipse you need to set those environment variables in the run configuration as illustrated in the figure below: Those variables are used the Spring boot application.configuration file, for example for the postgresql URL: spring.datasource.url= ${ POSTGRESQL_URL } When the Spring repository class establishes a TLS or SSL communication to the Postgresql service in IBM Cloud (or ICP), both client and server negotiate a stateful connection by using a handshaking procedure. During this procedure, the server usually sends back its identification in the form of a digital certificate. Java programs store certificates in a repository called Java KeyStore (JKS). To create the SSL connection you will need to make the server public certificate available to your Java client JVM. The certificate is persisted in a Trustore. So get the server public certificate: postgresql.crt , then convert it to a form Java understands. To download a text version of the base64 certificate as defined in the connection credentials of the IBM Cloud postgressql service use the following command: $ ibmcloud cdb deployment-cacert Green-DB-PostgreSQL # if you do not have the cloud database plugin does the following and rerun previous command: $ ibmcloud plugin install cloud-databases Save the certificate to a file (e.g. postgressql.crt). Then transform it to a Java format and save it to a keystore used by Java # transform $ openssl x509 -in postgressql.crt -out postgressql.crt.der -outform der # save in keystore $ keytool -keystore clienttruststore -alias postgresql -import -file postgressql.crt.der -storepass changeit Then adding the following setting will let the Java program accesses the certificates from the keystore. java -jar -Djavax.net.ssl.trustStore=clienttruststore -Djavax.net.ssl.trustStorePassword=changeit ./target/SpringContainerMS-1.0-SNAPSHOT.jar application.SBApplication We recommend reading this section of Postgresql product documentation, and this article from Baeldung on SSL handshake in Java. Now to make all this available in docker container we propose to let the previous two commands run within the Dockerfile during the build process.","title":"Security"},{"location":"springboot/#manually-deploy-to-iks","text":"The steps are the same as other project and we are providing the same type of tools: login to IBM Cloud - Adapt to your region. ibmcloud login -a https://api.us-east.bluemix.net connect to the IKS cluster ibmcloud ks region-set us-east ibmcloud ks cluster-config fabio-wdc-07 Define the Kube config variable so all the kubectl and helm commands will contact the good cluster. export KUBECONFIG= ~/.bluemix/plugins/container-service/clusters/fabio-wdc-07/kube-config-wdc07-fabio-wdc-07.yml login to the private registry ibmcloud cr login build the application jar and docker image using our script ./script/buildDocker.sh IBMCLOUD Push the docker image to the private registry docker push us.icr.io/ibmcaseeda/kc-springcontainerms Deploy the Helm release ./scripts/deployHelm which is equivalent to the command: helm install $chart/ --name $kname --namespace $ns If for any reason you get a \"image can't be pulled\" error. Verify the image is in the private registry with the command: docker cr image-list Then if the image exists, verify there is a secret defined to keep the security key to access the registry in the same namespace as you are deploying your app, and this secret is referenced in the yaml file: \"imagePullSecrets\": [ { \"name\": \"browncompute-registry-secret\" } ],","title":"Manually deploy to IKS"},{"location":"springboot/#perform-integration-tests-from-local-to-remote-ibm-cloud-deployment","text":"As presented in the section above , we have a set of tools that can create container, orders and verify the code is running well in its complete deployed environment. The same scripts can be run with the IBMCLOUD argument. So below is the list of command to perform on you laptop while the Spring boot app is deployed on IKS, using Event Streams and Postgresql on IBM Cloud: Start a container consumer to trace the execution: in the folder refarch-kc/itg-tests/ContainersPython . It listens to get a container with id: \"ic-c01\" ./runContainerConsumer.sh IBMCLOUD ic-c01 Run a producer of container events to add a new container into the inventory: in the same folder refarch-kc/itg-tests/ContainersPython ./addContainer.sh IBMCLOUD ic-c01 Start an order consumer, to trace the traffic: in the folder refarch-kc/itg-tests/OrdersPython ./runOrderConsumer.sh IBMCLOUD order01 Create an order from the same folder: refarch-kc/itg-tests/OrdersPython addOrder.sh IBMCLOUD order01 Going to the web URL of the container service you should see the container added.","title":"Perform integration tests from local to remote IBM Cloud deployment"},{"location":"springboot/#encountered-issues-with-some-solution","text":"The level of abstraction in Spring is nice when doing basic things but can become a nightmare when doing real bigger application including different libraries. Also migrating or using the last version 2.xx, bring changes to existing code and tests. Below is a list of iisues we spent time on: When JPA started, it creates / updates database schema, and for example enforced to have an Id as int while it was as string. As a bypass we create the table before in postsgresql using psql tool. When starting the spring data, JPA will try to connect to the PostgresSQL and execute a set of validation, one of them could generate the following exception: Method org.postgresql.jdbc.PgConnection.createClob() is not yet implemented . The explanation for the solution is here. When running the SpringContainer inside docker-compose the following exception may occur org.postgresql.util.PSQLException: Connection to postgres:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections while running the same code with maven using the same local backend works. Testing endpoint /health did not work on Spring 2.1.4. This is due a new Actuator capabilities ( spring-boot-starter-actuator ) that adds endpoints to manage a webapp in production. This is a nice feature. So remove any hold Health api class and modify the url to /actuator/health . Be sure to read this note on actuator. Deployed in Kubernetes service, the pod has issue on postgress SSL handshake. (org.postgresql.util.PSQLException: SSL error: Received fatal alert: handshake_failure). SSL handshakes are a mechanism by which a client and server establish the trust and logistics required to secure their connection over the network. This problem may be linked to a SSL certificate not found or wrong encryption protocol. We need to be sure a Java Keystore is defined and includes the public certificate coming from the server. See security section above. When deploying to IKS, we did see an exception about container topics not visible . This problem is coming from the fact that we are using Event Stream simplest deployment, which is multi tenants. And even if our code reach the brokers, our api key was wrong so we could get to another instance which did not have our topics. Be sure the api key is well defined in the kubernetes secret. (see this note to know how to do it )","title":"Encountered issues with some solution"},{"location":"springboot/#references","text":"Spring Code generator Spring Kafka and the spring kafka documentation sptring Data and JPA SSL and postgresql Runninf postgresql in docker","title":"References"}]}